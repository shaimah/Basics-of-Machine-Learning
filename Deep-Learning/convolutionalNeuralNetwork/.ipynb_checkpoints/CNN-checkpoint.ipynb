{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "64bIrOv5rEYJ",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2e176d1ab1160888bd08953c1fec6598",
     "grade": false,
     "grade_id": "cell-559e06fab022e928",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Convolutional Neural Networks\n",
    "\n",
    "Although you can implement a neural network using only numpy library,. with adding more components to the network (activations, regularization, batchnorm, etc.), the code becomes very complex and difficult to manage. Using Pytorch allows us to modularize the components of a network and hide the complexity. \n",
    "\n",
    "We will build a neural network using PyTorch for FashionMnist data for Image classification. However, it would be impractical to build a deep neural network with only fully connected layers for images. Instead, we will build a Convolutional Neural Network, which is the most popular approach to extracting features from an image without too much computation (compared to a fully connected layer). \n",
    "\n",
    "**It will be ideal to solve this assignemnet on a computer with a GPU**. I personally used [Google Colab](https://www.youtube.com/watch?v=inN8seMm7UI&ab_channel=TensorFlow), but [Kaggle](https://www.kaggle.com/dansbecker/running-kaggle-kernels-with-a-gpu) is also an option.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "QNjuVk66rEYO",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6da914e441f04a8a66e6dceec29a35c4",
     "grade": false,
     "grade_id": "cell-98105dbd82868b57",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We first import the neccesary packages required to build the model. We also fix a seed through out the process to have a deterministic behaviour (reproduce same results) of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "editable": false,
    "id": "9v4EAhu1rEYO",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "19fe0393c09e9a63890d67116c2c72f2",
     "grade": false,
     "grade_id": "cell-675473914c4b9f68",
     "locked": true,
     "schema_version": 3,
     "solution": false
    },
    "outputId": "18f3e58f-45c7-4f5c-c054-1aba9431dea5"
   },
   "outputs": [],
   "source": [
    "## Imports\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import numpy.testing as npt\n",
    "#from torchsummary import summary\n",
    "# from tqdm import trange\n",
    "\n",
    "# Checks for the availability of GPU \n",
    "is_cuda = torch.cuda.is_available()\n",
    "if torch.cuda.is_available():\n",
    "    print(\"working on gpu!\")\n",
    "else:\n",
    "    print(\"No gpu! only cpu ;)\")\n",
    "    \n",
    "## The following random seeds are just for deterministic behaviour of the code and evaluation\n",
    "\n",
    "##############################################################################\n",
    "################### DO NOT MODIFY THE CODE BELOW #############################    \n",
    "##############################################################################\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed_all(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "\n",
    "############################################################################### \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dBP8tQblrEYQ",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7eb19f0b795a90d9575c1139d3a87e7c",
     "grade": false,
     "grade_id": "cell-9c87add622617c1e",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### STEP0. Setting up the DataLoaders\n",
    "\n",
    "In the following cell we will first download and arrange the data. FashionMNIST dataset is already available in the official PyTorch repository. Hence, the following cell checks for the availability of FashionMNIST data and downloads if the data is not available.   \n",
    "\n",
    "The following parts are already written for you to handle the data.\n",
    "- import neccesary pytorch packages for data handling.\n",
    "- We then move the data onto PyTorch tensors.  \n",
    "- Next we define the parameters like batch_size for data handling. A different batch_size for test data is used to     make sure that number of samples in the test data are perfectly divisible.\n",
    "- create dataloaders for training and testing data to iterate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "xueQ11uDrEYQ",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f9026479c9b2db3497126f8ec4a5868f",
     "grade": false,
     "grade_id": "cell-e518bb36bae17b30",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "\n",
    "if not os.path.isdir('./data'):\n",
    "    os.mkdir('./data')\n",
    "root = './data/'\n",
    "\n",
    "# List of transformation on the data - here we will normalize the image data to (-1,1)\n",
    "transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5), (0.5)),])\n",
    "# Geta  handle to Load the data\n",
    "training_data = torchvision.datasets.FashionMNIST(root, train=True, transform=transform,download=True)\n",
    "testing_data = torchvision.datasets.FashionMNIST(root, train=False, transform=transform,download=True)\n",
    "\n",
    "num_train = len(training_data)\n",
    "indices = list(range(num_train))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(0.2 * num_train))\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "train_bs = 60\n",
    "test_bs = 50\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "# Create Data loaders which we will use to extract minibatches of data to input to the network for training\n",
    "train_loader = torch.utils.data.DataLoader(training_data, batch_size=train_bs,\n",
    "    sampler=train_sampler, drop_last=False)\n",
    "valid_loader = torch.utils.data.DataLoader(training_data, batch_size=train_bs, \n",
    "    sampler=valid_sampler, drop_last=False)\n",
    "test_loader = torch.utils.data.DataLoader(testing_data, batch_size=test_bs, \n",
    "    drop_last=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "3CQY7vi4rEYQ",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6f5531a49684a81df0e0f59102be6227",
     "grade": false,
     "grade_id": "cell-f9b382eeb7593ce2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Visualize a Few Data Samples\n",
    "\n",
    "In the following cell we first peek into a random batch of images together with labels and visualize them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 485
    },
    "deletable": false,
    "editable": false,
    "id": "3aQM8-KfrEYR",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "87e6c7dfdb9d4d57ab2421fad9e63cb9",
     "grade": false,
     "grade_id": "cell-1168a8b4140174bf",
     "locked": true,
     "schema_version": 3,
     "solution": false
    },
    "outputId": "d6088056-45b8-4771-b5dd-b39e4d15d2fd"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "## get a batch of data\n",
    "images, labels = iter(train_loader).next()\n",
    "\n",
    "\n",
    "image_dict = {0:'T-shirt/Top', 1:'Trouser', 2:'Pullover', 3:'Dress',\n",
    "              4:'Coat', 5:'Sandal', 6:'Shirt', 7:'Sneaker',\n",
    "              8:'Bag', 9:'Ankle Boot'}\n",
    "\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "\n",
    "print(images.size())\n",
    "\n",
    "for i in np.arange(1, 13):\n",
    "    ax = fig.add_subplot(3,4,i, frameon=False)\n",
    "    img = images[i][0]\n",
    "    ax.set_title(image_dict[labels[i].item()])\n",
    "    plt.imshow(img, cmap='gray')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "shsQOK9SrEYR",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6dcbe042b4711d11eafae3ec791e14b6",
     "grade": false,
     "grade_id": "cell-f48cb922457e6d1e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### STEP1. Definting the Architecture and Building the Model\n",
    "\n",
    "We implement a Convolutional Neural Network as our model. We make use of the following layers in our model.\n",
    "- a convolution layer for extracting features.\n",
    "- batchnorm layer for normalizing the weights in the hidden layers.\n",
    "- ReLU activation function for the non-linearity between layers.\n",
    "- Finally fully connected layers in the end.\n",
    "\n",
    "#### Model:\n",
    "\n",
    "we make use of the following convolutional neural network architecture for our dataset. \n",
    "\n",
    "- convolution layer output_channels-16 kernel_size=3 stride=1 padding-1\n",
    "- batchnormalization layer\n",
    "- ReLU activation layer\n",
    "- maxpool layer kernel_size=2 stride=2\n",
    "- convolution layer output_channels-32 kernel_size=3 stride=1 padding-1\n",
    "- batchnormalization layer\n",
    "- ReLU activation layer\n",
    "- maxpool layer kernel_size=2 stride=2\n",
    "- convolution layer output_channels-64 kernel_size=5 stride=1 padding-2\n",
    "- batchnormalization layer\n",
    "- ReLU activation layer\n",
    "- maxpool layer kernel_size=2 stride=2\n",
    "- fully connected layer - number_of_classes\n",
    "\n",
    "\n",
    "\n",
    "- We first define a class called Model nheriting from Pytorch's nn.Module.\n",
    "- In init(constructor), we define all the layers that are used to build the model\n",
    "- Define a forward function for a sequential model that takes in images as input and returns the predictions as output.\n",
    "\n",
    "All the functions are available in the PyTorch package. Read the documentation/source code for a better understanding.\n",
    "\n",
    "- Convolutional layer: https://pytorch.org/docs/stable/nn.html#convolution-layers\n",
    "- Batchnorm layer: https://pytorch.org/docs/stable/nn.html#normalization-layers\n",
    "- Activation ReLU: https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity\n",
    "- Maxpooling layer: https://pytorch.org/docs/stable/nn.html#pooling-layers\n",
    "- Fully connected layer: https://pytorch.org/docs/stable/nn.html#linear-layers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "BIJgXe8grEYS",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "517fa5c8accc5740302af5e2cfbcc415",
     "grade": false,
     "grade_id": "network_model_soln",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Model(nn.Module):\n",
    "    ## init function is the constructor and we define all the layers used in our model. \n",
    "    def __init__(self, num_classes=10):\n",
    "        super(Model, self).__init__()\n",
    "         # define a convolutional layer with 16 channels, kernel_size=3, stride=1 and padding=1\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride = 1, padding=1)\n",
    "\n",
    "        # define a batchnormalization layer \n",
    "        self.batch1 = nn.BatchNorm2d(16)\n",
    "\n",
    "        # define a relu layer\n",
    "        self.relu1 = nn.ReLU()\n",
    "\n",
    "        # define a maxpool layer with kernel_size=2, stride=2\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2,stride = 2)\n",
    "        \n",
    "        # define a convolutional layer with 32 channels, kernel_size=3, stride=1 and padding=1\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride = 1, padding=1)\n",
    "        \n",
    "        # define a batchnormalization layer \n",
    "        self.batch2 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        # define a relu layer\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "        # define a maxpool layer with kernel_size=2, stride=2\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride = 2)\n",
    "        \n",
    "        # define a convolution layer with 64 channels, kernel_size=5, stride=1 and padding=2\n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5, stride = 1, padding=2)\n",
    "              \n",
    "        # define a batchnorm layer        \n",
    "        self.batch3 = nn.BatchNorm2d(64)\n",
    "\n",
    "        # define a relu layer\n",
    "        self.relu3 = nn.ReLU()\n",
    "\n",
    "        # define a maxpool layer with kernel_size=2, stride=2\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride = 2)\n",
    "\n",
    "        # define a fully connected layer from resulting dimension -> number of classes\n",
    "        \n",
    "        self.fc = nn.Linear(64*3*3, num_classes)       \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        \n",
    "        # Conv 1\n",
    "        # We will start with feeding the data to the first layer. \n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.batch1(x)\n",
    "        x = self.relu1(x) \n",
    "        # We take the output x and feed it back to the next layer \n",
    "        \n",
    "        # Max Pool 1\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        # Conv 2\n",
    "        x = self.conv2(x)\n",
    "        x = self.batch2(x)\n",
    "        x = self.relu2(x)\n",
    "        \n",
    "        # Max Pool 2\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        # Conv 3\n",
    "        x = self.conv3(x)\n",
    "        x = self.batch3(x)\n",
    "        x = self.relu3(x)\n",
    "        \n",
    "        # Max Pool 3\n",
    "        x = self.pool3(x)     \n",
    "        \n",
    "    # Note that before the fully connected(fc) layer, the output is a feature map with size (N,C,H,W)\n",
    "    # but a fully connected layers expects a input of size (N, dimension). Therefore, before passing the output of\n",
    "    # maxpool layer to the fc layer, we must first flatten the output of previous layer to a size (N,C*H*W) \n",
    "    # and then pass it to the fully connected layer\n",
    "\n",
    "        x = x.flatten(x)\n",
    "        \n",
    "        # Linear Layer\n",
    "        x = self.fc(x)        \n",
    "        \n",
    "        return x        \n",
    "        \n",
    "    \n",
    "    # Note that before the fully connected(fc) layer, the output is a feature map with size (N,C,H,W)\n",
    "    # but a fully connected layers expects a input of size (N, dimension). Therefore, before passing the output of\n",
    "    # maxpool layer to the fc layer, we must first flatten the output of previous layer to a size (N,C*H*W) \n",
    "    # and then pass it to the fully connected layer, like x = x.flatten(x)\n",
    "    def flatten(self, x):\n",
    "        N, C, H, W = x.size()\n",
    "        x = x.reshape((N, C*H*W))                \n",
    "        \n",
    "        return x\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IvXGBtROrEYT"
   },
   "outputs": [],
   "source": [
    "# Setting up a few learning parameters\n",
    "learning_rate = 1e-2\n",
    "decayRate = 0.999\n",
    "epochs = 5\n",
    "number_of_classes = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XjHis8E-rEYT"
   },
   "source": [
    "### STEP2. Initialize the CNN Model\n",
    "\n",
    "Define a loss criterion, In this assignment we will use cross-entropy loss between the predictions and ground truth to estimate the loss. \n",
    "- CrossEntropyLoss - https://pytorch.org/docs/stable/nn.html#crossentropyloss\n",
    "\n",
    "We also define a optimization strategy to update the weights. In this assignment we use the most commonly used Adam optimizer from the PyTorch package.\n",
    "\n",
    "- Adam - https://pytorch.org/docs/stable/optim.html#algorithms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "id": "jqDuph8prEYT",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dff0b3cd47411f8bf3dc16268a3a7c6e",
     "grade": false,
     "grade_id": "optim_soln",
     "locked": false,
     "schema_version": 3,
     "solution": true
    },
    "outputId": "f221086c-5ff6-48f0-9afa-50a96d53a1ab"
   },
   "outputs": [],
   "source": [
    "## First we will define an instance of the model to train\n",
    "model = Model(num_classes=number_of_classes)\n",
    "print(model)\n",
    "\n",
    "#Move the model to the gpu if is_cuda\n",
    "if is_cuda:\n",
    "  model = model.cuda()\n",
    "\n",
    "# define the loss 'criterion' as nn.CrossEntropyLoss() object\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "# Initialize the Adam optimizer for the model.parameters() using the learning_rate\n",
    "optimizer = torch.optim.Adam(model.parameters(), learning_rate)\n",
    "\n",
    "# This is the learning rate scheduler. It decreases the learning rate as we approach convergence\n",
    "my_lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=decayRate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wqD65PCSrEYU",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4e52cd64b766001f7527c8f64e0a0870",
     "grade": false,
     "grade_id": "cell-1e2e5baffbc2c303",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### STEP3. Training the Model\n",
    "\n",
    "The training loop is setup in the following way:\n",
    "\n",
    "For every batch in the defined number of epochs\n",
    "\n",
    "- Move the images and labels to the gpu by checking is_cuda\n",
    "- Extract output by passing images through the model \n",
    "- pass the output and ground truth to the loss criterion for batch loss\n",
    "- clear the gradients \n",
    "- backpropagate (compute gradients w.r.t the parameters) using backward() \n",
    "- update the parameters with a single optimization step\n",
    "- update the training loss for plots\n",
    "\n",
    "repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "t3CtvtqQrEYU",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "06673cae7788c67677e8deda3b73c167",
     "grade": false,
     "grade_id": "train_model_soln",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "## training loop \n",
    "\n",
    "## Number of epochs the model runs\n",
    "\n",
    "def train_model(epochs=25, validate=True):\n",
    "    '''\n",
    "    A function to train the model on the dataset and returns the trained model, training loss and\n",
    "    validation loss for every epoch.\n",
    "    \n",
    "    Inputs:\n",
    "        epochs: Number of times the model should be trained on the whole data.\n",
    "        validate: A boolean parameter that validates on validation data.\n",
    "        \n",
    "    Outputs:\n",
    "        model: The model trained for specified number of epochs\n",
    "        training loss: A list of training losses computed for every epoch.\n",
    "        validation loss: A list of validation losses computed for every epoch.\n",
    "    \n",
    "    '''\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Iterate through the batches in the data\n",
    "        training_loss = 0.0\n",
    "        validation_loss = 0.0\n",
    "        \n",
    "        model.train()\n",
    "        itr = 0\n",
    "        for (images,labels)  in train_loader:\n",
    "            # Move the images and labels to the gpu by checking is_cuda\n",
    "            if is_cuda:\n",
    "              images = images.cuda()\n",
    "              labels = labels.cuda()\n",
    "            # Extract outputs by passing images through the model\n",
    "            outputs = model(images)\n",
    "            \n",
    "  \n",
    "            # estimate loss using criterion(.) with 'outputs' and 'labels'\n",
    "            loss = criterion(outputs, labels)\n",
    "            # clear the gradients with .zero_grad()\n",
    "            optimizer.zero_grad()\n",
    "            # Estimate gradients using .backward()\n",
    "            loss.backward()\n",
    "            # update model with .step()\n",
    "            optimizer.step()\n",
    "            # update learning_rate with with a .step using my_lr_scheduler\n",
    "            my_lr_scheduler.step()\n",
    "\n",
    "            # add loss to training_loss\n",
    "            training_loss += loss  \n",
    "            \n",
    "            if itr%100 == 0:\n",
    "                print('Epoch %d/%d, itr = %d, Train Loss = %.3f, LR = %.3E'\\\n",
    "                      %(epoch, epochs, itr, loss.item(),optimizer.param_groups[0]['lr']))\n",
    "            itr += 1\n",
    "        train_loss.append(training_loss/len(train_loader))\n",
    "        print('------------------------------------------------')\n",
    "        \n",
    "        if validate:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                itr = 0\n",
    "                for (images,labels)  in valid_loader:\n",
    "                    # During validation we do not update gradients. We set model to eval()\n",
    "                    model.eval()\n",
    "                    # Move images and labels to gpu if is_cuda\n",
    "                    if is_cuda:\n",
    "                      images = images.cuda()\n",
    "                      labels = labels.cuda()\n",
    "                    # get 'outputs' using model and images\n",
    "                    outputs = model(images)\n",
    "                    # estimate loss using criterion(.) with 'outputs' and 'labels'\n",
    "                    loss = criterion(outputs,labels)\n",
    "                    # add loss to 'validation_loss'\n",
    "                    validation_loss += loss                           \n",
    "                    if itr%100 == 0:\n",
    "                        print('Epoch %d/%d, itr = %d, Val Loss = %.3f, LR = %.3E'\\\n",
    "                              %(epoch, epochs, itr, loss.item(),optimizer.param_groups[0]['lr']))\n",
    "                    itr += 1\n",
    "                val_loss.append(validation_loss/len(valid_loader))\n",
    "                print('################################################')\n",
    "                \n",
    "    return model, train_loss, val_loss\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "editable": false,
    "id": "FP0Z9aPyrEYU",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9850cf853a8d0e7a84387bbb72f8c10f",
     "grade": false,
     "grade_id": "cell-3ef122427f827977",
     "locked": true,
     "schema_version": 3,
     "solution": false
    },
    "outputId": "0e474f81-2a3c-4090-f8a4-f04c68a4cfc5"
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "trained_model, train_loss, val_loss = train_model(epochs, validate=True)\n",
    "end = time.time()\n",
    "print('Time to train in seconds ',(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "deletable": false,
    "editable": false,
    "id": "Z5Yy9QCurEYV",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8e12aa54daa529184ce87590889347cc",
     "grade": false,
     "grade_id": "cell-35244d9978da3c81",
     "locked": true,
     "schema_version": 3,
     "solution": false
    },
    "outputId": "8d9bafb9-2bf4-400c-a7b9-7d02f844189a"
   },
   "outputs": [],
   "source": [
    "# Plot the losses\n",
    "it = np.arange(epochs)\n",
    "plt.plot(it, train_loss, label='training loss')\n",
    "plt.plot(it, val_loss, label='validation loss')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('losses')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "id": "nH_GcUV3rEYV",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bfa73d763cb1ddbd42d8ff1548d42e1c",
     "grade": false,
     "grade_id": "cell-fe85b51930497822",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### STEP4. Testing the Classification\n",
    "\n",
    "In the testing loop we don't update the weights. The trained model is tested for all the samples in test data to compute the accuracy and observe how well the model is generalizing to the unseen data. \n",
    "\n",
    "The testing loop is setup in the following way: \n",
    "\n",
    "For every batch in the testing data\n",
    "\n",
    "- Put the model in the evaluation mode and turn off the gradients\n",
    "- Move the images and labels to the device available\n",
    "- extract output from the model for the input\n",
    "- compute the prediction class by choosing the one with maximum probability in the predictions.\n",
    "- Compare the prediction classes with true classes.\n",
    "- calculate accuracy\n",
    "- update test_loss for plots\n",
    "\n",
    "repeat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "id": "VwErQNu3rEYV",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6f32a3ca4f639cbfd8e5c64aa8775d11",
     "grade": false,
     "grade_id": "test_model_soln",
     "locked": false,
     "schema_version": 3,
     "solution": true
    },
    "outputId": "ed7019cd-9fc8-408e-f5cc-b7865fea3417"
   },
   "outputs": [],
   "source": [
    "## Testing Loop\n",
    "\n",
    "def evaluate_model(model, loader):\n",
    "    '''\n",
    "    A function to test the trained model on the dataset and print the accuracy on the testset.\n",
    "    \n",
    "    Inputs:\n",
    "        model: Trained model\n",
    "        loader: train_loader or test_loader\n",
    "        \n",
    "    outputs:\n",
    "        accuracy. returns the accuracy of prediction\n",
    "    '''\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total_samples = 0\n",
    "        for images, labels in loader:\n",
    "            ## Move the images and labels to gpu if is_cuda \n",
    "            if is_cuda:\n",
    "              images = images.cuda()\n",
    "              labels = labels.cuda()\n",
    "            \n",
    "            ## Get the output of the model by passing images as input to the model\n",
    "            preds = []\n",
    "            outputs = model(images)\n",
    "            ## estimate the index of the highest output in each row, that is the predicted label \n",
    "            preds.extend(outputs.argmax(axis=1).cpu().numpy())\n",
    "\n",
    "            ## There is no need for softmax if the goal is to merely identify the predicted category\n",
    "\n",
    "            ## compare predictions with ground truth for number of correct samples\n",
    "            correct += (outputs.argmax(1) == labels).type(torch.float).sum().item()\n",
    "            \n",
    "            total_samples += labels.size(0)\n",
    "        \n",
    "        accuracy = correct/total_samples*100\n",
    "        print(\"Total Accuracy on the Input set: {} %\".format(accuracy))\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d0809b359b485a5d3c607c767fd2648e",
     "grade": false,
     "grade_id": "cell-99bb2d4e6a095086",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# With these settings, obtained 95% train and 91% test accuracy\n",
    "tr_acc = evaluate_model(model, train_loader)\n",
    "ts_acc = evaluate_model(model, test_loader)\n",
    "print('Train Accuracy = %.3f'%(tr_acc))\n",
    "print('Test Accuracy = %.3f'%(ts_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 879
    },
    "deletable": false,
    "editable": false,
    "id": "9_MkVg6ErEYW",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b4ece53bc914981efc7566f454d2c571",
     "grade": false,
     "grade_id": "cell-06766329c9d4e874",
     "locked": true,
     "schema_version": 3,
     "solution": false
    },
    "outputId": "d33b518a-1139-4a50-d1ed-4c32ed85adc6"
   },
   "outputs": [],
   "source": [
    "## Visualize the test samples with predicted output and true output\n",
    "images, labels = iter(test_loader).next()\n",
    "# images = images.numpy()\n",
    "if is_cuda:\n",
    "  images = images.cuda()\n",
    "  labels = labels.cuda()\n",
    "\n",
    "out = model(images)\n",
    "_, preds = torch.max(out, dim=1)\n",
    "\n",
    "images = images.to('cpu').numpy()\n",
    "\n",
    "fig = plt.figure(figsize=(15,15))\n",
    "for i in np.arange(1, 13):\n",
    "    ax = fig.add_subplot(4, 3, i)\n",
    "    plt.imshow(images[i][0])\n",
    "    ax.set_title(\"Predicted: {}/ Actual: {}\".format(image_dict[preds[i].item()], image_dict[labels[i].item()]), \n",
    "                color=('green' if preds[i] == labels[i] else 'red'))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "celltoolbar": "Edit Metadata",
  "colab": {
   "collapsed_sections": [],
   "name": "assignment_4.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
