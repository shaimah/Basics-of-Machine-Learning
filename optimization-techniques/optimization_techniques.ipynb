{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "97b470e14726ef3628924dde59f4647a",
     "grade": false,
     "grade_id": "cell-1a02bff32a097b76",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Optimization Techniques\n",
    "\n",
    "We will build a multilayer neural network and train it to classify hand-written digits into 10 classes (digits 0-9). However, the focus of this assignment will be introducing optmization techniques such as dropout, momentum and learning_rate scheduling and use of minibatch gradient descent. The effect of such optimization techniques is reflected in the model's accuracy, which is significantly improved compared to building an model without any optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bd5ac0db59406e661d8ef88be3baa17a",
     "grade": false,
     "grade_id": "cell-d9bd60ff8a7a5aba",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#import libraries and functions to load the data\n",
    "from digits import get_mnist\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import ast\n",
    "import sys\n",
    "import numpy.testing as npt\n",
    "import pytest\n",
    "import random\n",
    "from IPython.core.debugger import set_trace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2d8b8402ab293c269aef902b1afe21ca",
     "grade": false,
     "grade_id": "cell-7e789c2d07d0df38",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### STEP0. Load and Visualize Data\n",
    "\n",
    "MNIST dataset contains grayscale samples of handwritten digits of size 28 $\\times$ 28. It is split into training set of 60,000 examples, and a test set of 10,000 examples. We will use the entire dataset for training. Since we plan to use minibatch gradient descent, we can work with a larger dataset and not worry if it will fit into memory. You will also see the improved speed of minibatch gradient descent compared to when we used batch gradeint descent (using the entire training data as a batch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8ca9098028d932a01a5cc12ac78b384e",
     "grade": false,
     "grade_id": "cell-153e3e96f279c5f5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "random.seed(1)\n",
    "np.random.seed(1)\n",
    "trX, trY, tsX, tsY = get_mnist()\n",
    "# We need to reshape the data everytime to match the format (d,m), where d is dimensions (784) and m is number of samples\n",
    "trX = trX.reshape(-1, 28*28).T\n",
    "trY = trY.reshape(1, -1)\n",
    "tsX = tsX.reshape(-1, 28*28).T\n",
    "tsY = tsY.reshape(1, -1)\n",
    "    \n",
    "# Lets examine the data and see if it is normalized\n",
    "print('trX.shape: ', trX.shape)\n",
    "print('trY.shape: ', trY.shape)\n",
    "print('tsX.shape: ', tsX.shape)\n",
    "print('tsY.shape: ', tsY.shape)\n",
    "print('Train max: value = {}, Train min: value = {}'.format(np.max(trX), np.min(trX)))\n",
    "print('Test max: value = {}, Test min: value = {}'.format(np.max(tsX), np.min(tsX)))\n",
    "print('Unique labels in train: ', np.unique(trY))\n",
    "print('Unique labels in test: ', np.unique(tsY))\n",
    "\n",
    "# Let's visualize a few samples and their labels from the train and test datasets.\n",
    "print('\\nDisplaying a few samples')\n",
    "visx = np.concatenate((trX[:,:50],tsX[:,:50]), axis=1).reshape(28,28,10,10).transpose(2,0,3,1).reshape(28*10,-1)\n",
    "visy = np.concatenate((trY[:,:50],tsY[:,:50]), axis=1).reshape(10,-1)\n",
    "    \n",
    "print('labels')\n",
    "print(visy)\n",
    "plt.figure(figsize = (8,8))\n",
    "plt.axis('off')\n",
    "plt.imshow(visx, cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7221fc5c8438626f55fed907bdbdeb70",
     "grade": false,
     "grade_id": "cell-36f264bd171e66a2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Rectified Linear Unit-ReLU\n",
    "\n",
    "ReLU (Rectified Linear Unit) is a piecewise linear function defined as\n",
    "\\begin{equation*}\n",
    "ReLU(Z) = \\text{max}(0,Z)\n",
    "\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "305c954ece9e4a95d802995ee253af50",
     "grade": false,
     "grade_id": "test_case1_relu_soln",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    '''\n",
    "    Computes relu activation of input Z\n",
    "    \n",
    "    Inputs: \n",
    "        Z: numpy.ndarray (n, m) which represent 'm' samples each of 'n' dimension\n",
    "        \n",
    "    Outputs: \n",
    "        A: where A = ReLU(Z) is a numpy.ndarray (n, m) representing 'm' samples each of 'n' dimension\n",
    "        cache: a dictionary with {\"Z\", Z}\n",
    "        \n",
    "    '''\n",
    "    cache = {\"Z\": Z}\n",
    "    A = np.maximum(0, cache[\"Z\"])\n",
    "    A = A.tolist()\n",
    "    \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0f49ae7f620c77d0e403a032c633e256",
     "grade": false,
     "grade_id": "cell-9e69ac398fc920e4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### ReLU - Gradient\n",
    "\n",
    "The gradient of ReLu($Z$) is 1 if $Z>0$ else it is 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dc140b793336a33c53c1fe2d27397f57",
     "grade": false,
     "grade_id": "test_case2_relu_der_soln",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def relu_der(dA, cache):\n",
    "    '''\n",
    "    Computes derivative of relu activation\n",
    "    \n",
    "    Inputs: \n",
    "        dA: derivative from the subsequent layer of dimension (n, m). \n",
    "            dA is multiplied elementwise with the gradient of ReLU\n",
    "        cache: dictionary with {\"Z\", Z}, where Z was the input \n",
    "            to the activation layer during forward propagation\n",
    "        \n",
    "    Outputs: \n",
    "        dZ: the derivative of dimension (n,m). It is the elementwise \n",
    "            product of the derivative of ReLU and dA\n",
    "        \n",
    "    '''\n",
    "    dZ = np.array(dA, copy=True)\n",
    "    Z = cache[\"Z\"]\n",
    "    \n",
    "    dZ[Z > 0] = 1\n",
    "    dZ[Z <= 0] = 0\n",
    "\n",
    "    dZ = np.multiply(dA,dZ)    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bd03aee42ab92a4b63da967f3759c88e",
     "grade": false,
     "grade_id": "cell-ff93df0fc4bbc430",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Linear activation and its derivative\n",
    "\n",
    "There is no activation involved here. It is an identity function. \n",
    "\\begin{equation*}\n",
    "\\text{Linear}(Z) = Z\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "695ee8c0246dcc69a45cdcc2b32e07cb",
     "grade": false,
     "grade_id": "cell-5c19d5fd5d97fb3e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def linear(Z):\n",
    "    '''\n",
    "    Computes linear activation of Z\n",
    "    This function is implemented for completeness\n",
    "        \n",
    "    Inputs: \n",
    "        Z: numpy.ndarray (n, m) which represent 'm' samples each of 'n' dimension\n",
    "        \n",
    "    Outputs: \n",
    "        A: where A = Linear(Z) is a numpy.ndarray (n, m) representing 'm' samples each of 'n' dimension\n",
    "        cache: a dictionary with {\"Z\", Z}   \n",
    "    '''\n",
    "    A = Z\n",
    "    cache = {}\n",
    "    cache[\"Z\"] = Z\n",
    "    return A, cache\n",
    "\n",
    "\n",
    "def linear_der(dA, cache):\n",
    "    '''\n",
    "    Computes derivative of linear activation\n",
    "    This function is implemented for completeness\n",
    "    \n",
    "    Inputs: \n",
    "        dA: derivative from the subsequent layer of dimension (n, m). \n",
    "            dA is multiplied elementwise with the gradient of Linear(.)\n",
    "        cache: dictionary with {\"Z\", Z}, where Z was the input \n",
    "            to the activation layer during forward propagation\n",
    "        \n",
    "    Outputs: \n",
    "        dZ: the derivative of dimension (n,m). It is the elementwise \n",
    "            product of the derivative of Linear(.) and dA\n",
    "    '''      \n",
    "    dZ = np.array(dA, copy=True)\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ce35f78ff70d66940badc942b348ebf3",
     "grade": false,
     "grade_id": "cell-076c0de6c87fa8af",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Softmax Activation and Cross-entropy Loss Function \n",
    "The softmax activation is computed on the outputs from the last layer and the output label with the maximum probablity is predicted as class label. The softmax function can also be refered as normalized exponential function which takes a vector of $n$ real numbers as input, and normalizes it into a probability distribution consisting of $n$ probabilities proportional to the exponentials of the input numbers.\n",
    "\n",
    "The input to the softmax function is the $(n \\times m)$ matrix, $ Z = [ z^{(1)} , z^{(2)}, \\ldots, z^{(m)} ] $, where $z^{(i)}$ is the $i^{th}$ sample of $n$ dimensions. We estimate the softmax for each of the samples $1$ to $m$. The softmax activation for sample $z^{(i)}$ is $a^{(i)} = \\text{softmax}(z^{(i)})$, where the components of $a^{(i)}$ are,\n",
    "\\begin{equation}\n",
    "a_k{(i)} = \\frac{\\text{exp}(z^{(i)}_k)}{\\sum_{k = 1}^{n}\\text{exp}(z^{(i)}_k)} \\qquad \\text{for} \\quad 1\\leq k\\leq n\n",
    "\\end{equation}\n",
    "\n",
    "The output of the softmax is $ A = [ a^{(1)} , a^{(2)} .... a^{(m)} ]$, where $a^{(i)} = [a^{(i)}_1,a^{(i)}_2, \\ldots, a^{(i)}_n]^\\top$.  In order to avoid floating point overflow, we subtract a constant from all the input components of $z^{(i)}$ before calculating the softmax. This constant is $z_{max}$, where, $z_{max} = \\text{max}(z_1,z_2,...z_n)$. The activation is given by,\n",
    "\n",
    "\\begin{equation}\n",
    "a_k{(i)} = \\frac{\\text{exp}(z^{(i)}_k- z_{max})}{\\sum_{k = 1}^{n}\\text{exp}(z^{(i)}_k - z_{max})} \\qquad \\text{for} \\quad 1\\leq k\\leq n\n",
    "\\end{equation}\n",
    "\n",
    "If the output of softmax is given by $A$ and the ground truth is given by $Y = [ y^{(1)} , y^{(2)}, \\ldots, y^{(m)}]$, the cross entropy loss between the predictions $A$ and groundtruth labels $Y$ is given by,\n",
    "\n",
    "\\begin{equation}\n",
    "Loss(A,Y) = - \\frac{1}{m} \\sum_{i=1}^m \\sum_{k=1}^{n}I \\{ y^i = k \\} \\text{log}a_k^i\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "where $I$ is the identity function given by \n",
    "\n",
    "\\begin{equation}\n",
    "I\\{\\text{condition}\\} = 1, \\quad \\text{if condition = True}\\\\\n",
    "I\\{\\text{condition}\\} = 0, \\quad \\text{if condition = False}\\\\\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "96ce0af0b0c8a2cbe0176be9a65aaf55",
     "grade": false,
     "grade_id": "test_case3_softmax_soln",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def softmax_cross_entropy_loss(Z, Y=np.array([])):\n",
    "    '''\n",
    "    Computes the softmax activation of the inputs Z\n",
    "    Estimates the cross entropy loss\n",
    "\n",
    "    Inputs: \n",
    "        Z: numpy.ndarray (n, m)\n",
    "        Y: numpy.ndarray (1, m) of labels\n",
    "            when y=[] loss is set to []\n",
    "    \n",
    "    Outputs:\n",
    "        A: numpy.ndarray (n, m) of softmax activations\n",
    "        cache: a dictionary to store the activations which will be used later to estimate derivatives\n",
    "        loss: cost of prediction\n",
    "    '''\n",
    "    \n",
    "    A = np.exp(Z-np.max(Z, axis = 0))/np.sum(np.exp(Z-np.max(Z, axis = 0)), axis = 0)\n",
    "    cache = {}\n",
    "    cache[\"A\"] = A\n",
    "\n",
    "    if len(Y) == 0:\n",
    "        loss = []\n",
    "    else:\n",
    "        loss = 1/Z.shape[1]*np.sum(-np.log(A[Y.astype(int),range(Z.shape[1])]))\n",
    "        \n",
    "    return A, cache, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cf8515684585cafb58a9d922e3a8720b",
     "grade": false,
     "grade_id": "cell-5151a9f9720ee789",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Derivative of the softmax_cross_entropy_loss(.)\n",
    "\n",
    "It is easier to directly estimate $dZ$ which is $\\frac{dL}{dZ}$, where $Z$ is the input to the *softmax_cross_entropy_loss($Z$)* function. \n",
    "\n",
    "Let $Z$ be the $(n\\times m)$ dimension input and $Y$ be the $(1,m)$ groundtruth labels. If $A$ is the $(n\\times m)$ matrix of softmax activations of $Z$, the derivative $dZ$ is given by, \n",
    "\n",
    "\\begin{equation}\n",
    "dZ =\\frac{1}{m} (A -\\bar{Y})\n",
    "\\end{equation}\n",
    "\n",
    "where, $\\bar{Y}$ is the one-hot representation of $Y$. \n",
    "\n",
    "One-hot encoding is a binary representation of the discrete class labels. For example, let $y^{(i)}\\in\\{0,1,2\\}$ for a 3-category problem. Assume there are $m=4$ data points. In this case $Z$ will be a $3 \\times 4$ matrix. Let the categories of the 4 data points be $Y=[1,0,1,2]$. The one hot representation is given by, \n",
    "\\begin{equation}\n",
    "\\bar{Y} = \n",
    "    \\begin{bmatrix}\n",
    "    0 ~ 1 ~ 0 ~ 0\\\\\n",
    "    1 ~ 0 ~ 1 ~ 0\\\\\n",
    "    0 ~ 0 ~ 0 ~ 1\n",
    "    \\end{bmatrix}\n",
    "\\end{equation}\n",
    "where, the one-hot encoding for label $y^{(1)} = 1$ is $\\bar{y}^{(1)} = [0, 1, 0]^\\top$. Similarly, the one-hot encoding for $y^{(4)} = 2$ is $\\bar{y}^{(4)} = [0, 0, 1]^\\top$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e131cd252e51deebb53d920f975a4ac5",
     "grade": false,
     "grade_id": "test_case4_softmax_der_soln",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def softmax_cross_entropy_loss_der(Y, cache):\n",
    "    '''\n",
    "    Computes the derivative of the softmax activation and cross entropy loss\n",
    "\n",
    "    Inputs: \n",
    "        Y: numpy.ndarray (1, m) of labels\n",
    "        cache: a dictionary with cached activations A of size (n,m)\n",
    "\n",
    "    Outputs:\n",
    "        dZ: derivative dL/dZ - a numpy.ndarray of dimensions (n, m) \n",
    "    '''\n",
    "    A = cache[\"A\"]\n",
    "    dZ = A.copy()\n",
    "    \n",
    "    #one-hot rep\n",
    "    dZ[Y.astype(int),range(Y.shape[1])] -= 1\n",
    "    dZ = dZ/Y.shape[1]\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP1. Dropout forward (first optimization technique)\n",
    "\n",
    "The dropout layer is introduced to improve regularization by reducing overfitting. The layer will zero out some of the activations in the input based on the 'drop_prob' value. Dropout is only appiled in 'train' mode and not in 'test' mode. In the 'test' mode the output activations are the same as input activations. \n",
    "We will implement the inverted droput method we discussed in the lecture. We define 'prob_keep' as the percentage of activations remaining after dropout, if drop_out = 0.3, then prob_keep = 0.7, i.e., 70% of the activations are retained after dropout. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6b4627af274670c942f2376fe2bac46f",
     "grade": false,
     "grade_id": "cell-e97aa827e6a861ff",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def dropout(A, drop_prob, mode='train'):\n",
    "        '''\n",
    "        Using the 'inverted dropout' technique to implement dropout regularization.\n",
    "        Inputs:\n",
    "            A: Activation input before dropout is applied - shape is (n,m)\n",
    "            drop_prob: dropout parameter. If drop_prob = 0.3, we drop 30% of the neuron activations\n",
    "            mode: Dropout acts differently in training and testing mode. Hence, mode is a parameter which\n",
    "                takes in only 2 values, 'train' or 'test'\n",
    "\n",
    "        Outputs:\n",
    "            A: Output of shape (n,m), with some values masked out and other values scaled to account for missing values\n",
    "            cache: a tuple which stores the drop_prob, mode and mask for use in backward pass.\n",
    "        '''\n",
    "        # When there is no dropout return the same activation\n",
    "        mask = None\n",
    "        if drop_prob == 0:\n",
    "            cache = (drop_prob, mode, mask)\n",
    "            return A, cache\n",
    "        \n",
    "        # The prob_keep is the percentage of activations remaining after dropout\n",
    "        # if drop_out = 0.3, then prob_keep = 0.7, i.e., 70% of the activations are retained\n",
    "        prob_keep = 1-drop_prob\n",
    "        \n",
    "        # Note: instead of a binary mask implement a scaled mask, where mask is scaled by dividing it \n",
    "        # by the prob_keep for example, if we have input activations of size (3,4), then the mask is \n",
    "        # mask = (np.random.rand(3,4)<prob_keep)/prob_keep\n",
    "        # We perform the scaling by prob_keep here so we don't have to do it specifically during backpropagation \n",
    "        # We then update A by multiplying it element wise with the mask\n",
    "        \n",
    "        if mode == 'train':\n",
    "            n, m = np.shape(A)\n",
    "            mask = (np.random.rand(n,m)<prob_keep) /prob_keep\n",
    "            A = A*mask\n",
    "                     \n",
    "        elif mode != 'test':\n",
    "            raise ValueError(\"Mode value not set correctly, set it to 'train' or 'test'\")\n",
    "        cache = (drop_prob, mode, mask)\n",
    "        \n",
    "        return A, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout backward\n",
    "\n",
    "In the backward pass, we estimate the derivative w.r.t. the dropout layer. We will need the 'drop_prob', 'mask' and 'mode' which is obtained from the cache saved during forward pass. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2672cdb298d304e4a8ddf9a77a195e81",
     "grade": false,
     "grade_id": "cell-34ce98b5a9658fae",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def dropout_der(dA_in, cache):\n",
    "        '''\n",
    "        Backward pass for the inverted dropout.\n",
    "        Inputs: \n",
    "            dA_in: derivative from the upper layers of dimension (n,m).\n",
    "            cache: tuple containing (drop_out, mode, mask), where drop_out is the probability of drop_out, \n",
    "                if drop_out=0, then the layer does not have any dropout,\n",
    "                mode is either 'train' or 'test' and \n",
    "                mask is a matirx of size (n,m) where 0's indicate masked values\n",
    "        Outputs:\n",
    "            dA_out = derivative of the dropout layer of dimension (n,m)\n",
    "        '''\n",
    "        \n",
    "        dA_out = None\n",
    "        drop_out, mode, mask = cache\n",
    "        # If there is no dropout return the same derivative from the previous layer\n",
    "        if not drop_out:\n",
    "            return dA_in\n",
    "        \n",
    "        # if mode is 'train' dA_out is dA_in multiplied element wise by mask\n",
    "        if mode == 'train':\n",
    "            dA_out = dA_in * mask\n",
    "            \n",
    "        # if mode is 'test' dA_out is same as dA_in\n",
    "        elif mode == 'test':\n",
    "            dA_out = dA_in \n",
    "            \n",
    "        return dA_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a23870ebbde5eeae58d49b8077aed39e",
     "grade": false,
     "grade_id": "cell-8ccc857eca28aa9d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### STEP2. Batchnorm forward (second optimization technique)\n",
    "\n",
    "Batchnorm scales the input activations in a minibatch to have a specific mean and variance allowing the training to use larger learning rates to improve training speeds and provide more stability to the training. During training, the input minibatch is first normalized by making it zero mean and scaled to unit variance, i.e., ($0,I$) normalized. The normalized data is then converted to have a mean ($\\beta$) and variance ($\\gamma$), i.e., ($\\beta,\\gamma I$) normalized. Here, $\\beta$ and $\\gamma$ are the parameters for the batchnorm layer which are updated during training using gradient descent. \n",
    "The original batchnorm paper was implemented by applying batchnorm before nonlinear activation. However, batchnorm has been found to be more effective when applied after activation. We will implement this version in Assignment 3. \n",
    "\n",
    "In the lecture, we also discussed implementation of batchnorm during test mode when a single sample may be input for evaluation. We will not be implementing this aspect of batchnorm for the assignment. This implementation will work as designed only when a minibatch of data is presented to the network during evaluation (test mode) and may not work as expected when a single image is input for evaluation (test mode). \n",
    "\n",
    "The batchnorm implementation is tricky, especially the backpropagation. You may use the following source for reference: [Batchnorm backpropagation Tutorial](https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html). Note: The tutorial representes data as a $(m,n)$ matrix, whereas we represent data as a $(n,m)$ matrix, where $n$ is feature dimensions and $m$ is number of samples. \n",
    "\n",
    "If you are unable to implement the batchnorm correctly, you can still get the network to work by setting the variable 'bnorm_list = [0,0,...,0,0]. This is a list of binary varibles indicating if batchnorm is used for a layer (0 means no batchorm for the corresponding layer). This variable is used in the 'multi_layer_network(.)' function when initalizing the network. \n",
    "Although the follwing testcases may fail, the network can still work without batchnorm and you can get partial credit. \n",
    "\n",
    "The variables you save into the cache is your choice. The tescase only tests for the normalized output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9bc0845e4a2333e69c031fe554c4e921",
     "grade": false,
     "grade_id": "batchnorm_forward_soln",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def batchnorm(A, beta, gamma):\n",
    "    '''\n",
    "    Batchnorm normalizes the input A to mean beta and standard deviation gamma\n",
    "    \n",
    "    Inputs: \n",
    "        A: Activation input after activation - shape is (n,m), m samples where each sample x is (n,1)\n",
    "        beta: mean vector which will be the center of the data after batchnorm - shape is (n,1)\n",
    "        gamma: standard deviation vector which will be scale of the data after batchnorm - shape (n,1)\n",
    "        \n",
    "    Outputs: \n",
    "        Anorm: Normalized version of input A - shape (n,m)\n",
    "        cache: Dictionary of the elements that are necessary for backpropagation\n",
    "    '''\n",
    "    \n",
    "    # When there is no batch norm for a layer, the beta and gamma will be empty arrays\n",
    "    if beta.size == 0 or gamma.size == 0:\n",
    "        cache = {}\n",
    "        return A, cache\n",
    "    # epsilon value used for scaling during normalization to avoid divide by zero. \n",
    "    # don't change this value - the test case will fail if you change this value\n",
    "    epsilon = 1e-5\n",
    "\n",
    "    n, m = np.shape(A)\n",
    "    #step1: calculate mean\n",
    "    mu = 1./m * np.sum(A, axis = 1)\n",
    "    mu = np.reshape(mu, (n,1))\n",
    "\n",
    "    #step2: subtract mean vector of every trainings example\n",
    "    xmu = A - mu\n",
    "    \n",
    "    #step3: following the lower branch - calculation denominator\n",
    "    sq = xmu ** 2\n",
    "    \n",
    "    #step4: calculate variance\n",
    "    var = 1./m * np.sum(sq, axis = 1)\n",
    "    var = np.reshape(var, (n,1))\n",
    "\n",
    "    \n",
    "    #step5: add eps for numerical stability, then sqrt\n",
    "    sqrtvar = np.sqrt(var + epsilon)\n",
    "    \n",
    "    #step6: invert sqrtwar\n",
    "    ivar = 1./sqrtvar\n",
    "    \n",
    "    #step7: execute normalization\n",
    "    xhat = xmu * ivar\n",
    "    \n",
    "    #step8: Nor the two transformation steps\n",
    "    gammax = gamma * xhat\n",
    "    \n",
    "    #step9\n",
    "    Anorm = gammax + beta\n",
    "    \n",
    "    #store intermediate\n",
    "    cache = cache = (xhat,gamma,xmu,ivar,sqrtvar,var,epsilon)\n",
    "        \n",
    "    return Anorm, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "235204f5e72f927c473528f0a1155a10",
     "grade": false,
     "grade_id": "cell-a8cf50ba49b92a32",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Batchnorm backward\n",
    "The forward propagation for batchnorm is relatively straightfoward to implement. For the backward propagation to worrk, you will need to save a set of variables in the cache during the forward propagation. The variables in your cache are your choice. The testcase only tests for the derivative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fb73a49523de475c649d69105c08ceac",
     "grade": false,
     "grade_id": "batchnorm_backward_soln",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def batchnorm_der(dA_in, cache):\n",
    "    '''\n",
    "    Derivative of the batchnorm\n",
    "    Inputs: \n",
    "        dA_in: derivative from the upper layers of dimension (n,m).\n",
    "        cache: Dictionary of the elements that are necessary for backpropagation\n",
    "    Outputs:\n",
    "        dA_out: derivative of the batchnorm layer of dimension (n,m)\n",
    "        dbeta: derivative of beta - shape (n,1)\n",
    "        dgamma: derivative of gamma - shape (n,1)\n",
    "    '''\n",
    "    # When the cache is empty, it indicates there was no batchnorm for the layer\n",
    "    if not cache:\n",
    "        dbeta = []\n",
    "        dgamma = []\n",
    "        return dA_in, dbeta, dgamma\n",
    "    \n",
    "    #unfold the variables stored in cache\n",
    "    xhat,gamma,xmu,ivar,sqrtvar,var,epsilon = cache\n",
    "    \n",
    "    #get the dimensions of the input/output\n",
    "    n,m = dA_in.shape\n",
    "    \n",
    "    #step9\n",
    "    dbeta = np.sum(dA_in, axis=1)\n",
    "    dbeta = np.reshape(dbeta, (n,1))\n",
    "    dgammax = dA_in #not necessary, but more understandable\n",
    "    \n",
    "    #step8\n",
    "    dgamma = np.sum(dgammax*xhat, axis=1)\n",
    "    dgamma = np.reshape(dgamma, (n,1))\n",
    "    dxhat = dgammax * gamma\n",
    "    \n",
    "    #step7\n",
    "    divar = np.sum(dxhat*xmu, axis=1)\n",
    "    divar = np.reshape(divar, (n,1))\n",
    "\n",
    "    dxmu1 = dxhat * ivar\n",
    "    \n",
    "    #step6\n",
    "    dsqrtvar = -1. /(sqrtvar**2) * divar\n",
    "    \n",
    "    #step5\n",
    "    dvar = 0.5 * 1. /np.sqrt(var+epsilon) * dsqrtvar\n",
    "    \n",
    "    #step4\n",
    "    dsq = 1. /m * np.ones((n,m)) * dvar\n",
    "    \n",
    "    #step3\n",
    "    dxmu2 = 2 * xmu * dsq\n",
    "    \n",
    "    #step2\n",
    "    dx1 = (dxmu1 + dxmu2)\n",
    "    dmu = -1 * np.sum(dxmu1+dxmu2, axis=1)\n",
    "    dmu = np.reshape(dmu, (n,1))\n",
    "    \n",
    "    #step1\n",
    "    dx2 = 1. /m * np.ones((n,m)) * dmu\n",
    "    \n",
    "    #step0\n",
    "    dA_out = dx1 + dx2    \n",
    "    \n",
    "    return dA_out, dbeta, dgamma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2afffb84ecba8a8f56c26aece3a4998c",
     "grade": false,
     "grade_id": "cell-2d445d2fe1bb530d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### STEP3. Parameter Initialization (third optimization technique)\n",
    "\n",
    "We will define the function to initialize the parameters of the multi-layer neural network.\n",
    "The network parameters will be stored as dictionary elements that can easily be passed as function parameters while calculating gradients during back propogation.\n",
    "\n",
    "The parameters are initialized using Kaiming He Initialization. For example, a layer with weights of dimensions $(n_{out}, n_{in})$, the parameters are initialized as\n",
    "$w = np.random.randn(n_{out},n_{in})*(2./np.sqrt(n_{in}))$ and \n",
    "$b = np.zeros((n_{out},1))$\n",
    "\n",
    "The dimension for weight matrix for layer $(l+1)$ is given by ( Number-of-neurons-in-layer-$(l+1)$   $\\times$   Number-of-neurons-in-layer-$l$ ). The dimension of the bias for for layer $(l+1)$ is (Number-of-neurons-in-layer-$(l+1)$   $\\times$   1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5776d8f512e36381fef5991f897725c0",
     "grade": false,
     "grade_id": "params_initialize_soln",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def initialize_network(net_dims, act_list, drop_prob_list):\n",
    "    '''\n",
    "    Initializes the parameters W's and b's of a multi-layer neural network\n",
    "    Adds information about dropout and activations in each layer\n",
    "    \n",
    "    Inputs:\n",
    "        net_dims: List containing the dimensions of the network. The values of the array represent the number of nodes in \n",
    "        each layer. For Example, if a Neural network contains 784 nodes in the input layer, 800 in the first hidden layer, \n",
    "        500 in the secound hidden layer and 10 in the output layer, then net_dims = [784,800,500,10]. \n",
    "        act_list: list of strings indicating the activation for a layer\n",
    "        drop_prob_list: list of dropout probabilities for each layer \n",
    "    \n",
    "    Outputs:\n",
    "        parameters: dictionary of \n",
    "                    {\"numLayers\":..}\n",
    "                    activations, {\"act1\":\"..\", \"act2\":\"..\", ...}\n",
    "                    dropouts, {\"dropout1\": .. , \"dropout2\": .., ...}\n",
    "                    network parameters, {\"W1\":[..],\"b1\":[..],\"W2\":[..],\"b2\":[..],...}\n",
    "            The weights are initialized using Kaiming He et al. Initialization\n",
    "    '''\n",
    "    net_dims_len = len(net_dims)\n",
    "    parameters = {}\n",
    "    parameters['numLayers'] = net_dims_len - 1;\n",
    "    for l in range(net_dims_len-1):\n",
    "        parameters[\"act\"+str(l+1)] = act_list[l]\n",
    "        parameters[\"dropout\"+str(l+1)] = drop_prob_list[l]\n",
    "        \n",
    "        # Note: Use He et al. Initialization to initialize W and set bias to 0's\n",
    "        parameters[\"W\"+str(l+1)] = np.random.randn(net_dims[l+1],net_dims[l])*(2./np.sqrt(net_dims[l]))\n",
    "        parameters[\"b\"+str(l+1)] = np.zeros((net_dims[l+1],1))        \n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e02e8f835aa4d3015ff23b01f6fc4a1e",
     "grade": false,
     "grade_id": "cell-ffe90db2e8c100f5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### STEP4. Adam (momentum) Parameters - Velocity and Gradient-Squares Initialization (fourth optimization technique)\n",
    "\n",
    "We will optmize using Adam (momentum). This requires velocity parameters $V$ and Gradient-Squares parameters $G$. Here is a quick recap of Adam optmization, \n",
    "\n",
    "\\begin{equation} \n",
    "V_{t+1} = \\beta V_{t} +(1-\\beta)\\nabla J(\\theta_t)\\\\ \n",
    "G_{t+1} = \\beta_2 G_{t} +(1-\\beta_2)\\nabla J(\\theta_t)^2\\\\ \n",
    "\\theta_{t+1} =\\theta_{t} -\\frac{\\alpha}{\\sqrt{G_{t+1}+\\epsilon}}V_{t+1}, \\quad \\theta \\in \\{ W,b \\} \n",
    "\\end{equation}\n",
    "\n",
    "Parameters $V$ are the momentum velocity parameters and parameters $G$ are the Gradient-Squares. \n",
    "$\\nabla J(\\theta)$ is the gradient term $dW$ or $db$, and $\\nabla  J(\\theta)^2$ is the element wise square of the gradient. \n",
    "$\\alpha$ is the step_size for gradient descent. It is has been estimated by decaying the 'learning_rate' based on 'decay_rat'e and 'epoch' number. $\\beta$, $\\beta_2$ and $\\epsilon$ are constants which we will set up later.\n",
    "\n",
    "Each of the parameters $W$'s and $b$'s for all the layers will have their corresponding velocity ($V$) and Gradient-Squares ($G$) parameters. The following function will initialize $V$ and $G$ to zeros with the same size as the corresponding parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e57ee4eb4685673bb0629cc87fcf2475",
     "grade": false,
     "grade_id": "cell-dd58518f843ee3ec",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def initialize_velocity(parameters, apply_momentum=True):\n",
    "    '''\n",
    "    The function will add Adam momentum parameters, Velocity and Gradient-Squares \n",
    "    to the parameters for each of the W's and b's \n",
    "    \n",
    "    Inputs: \n",
    "        parameters: dictionary containing, \n",
    "                    {\"numLayers\":..}\n",
    "                    activations, {\"act1\":\"..\", \"act2\":\"..\", ...}\n",
    "                    dropouts, {\"dropout1\": .. , \"dropout2\": .., ...}\n",
    "                    network parameters, {\"W1\":[..],\"b1\":[..],\"W2\":[..],\"b2\":[..],...}\n",
    "                    Note: It is just one dictionary (parameters) with all these key value pairs, not multiple dictionaries\n",
    "        apply_momentum: boolean on whether to apply momentum\n",
    "        \n",
    "    Outputs:\n",
    "        parameters: dictionary that has been updated to include velocity and Gradient-Squares. It now contains,\n",
    "                    {\"numLayers\":..}\n",
    "                    activations, {\"act1\":\"..\", \"act2\":\"..\", ...}\n",
    "                    dropouts, {\"dropout1\": .. , \"dropout2\": .., ...}\n",
    "                    {\"apply_momentum\":..}\n",
    "                    velocity parameters, {\"VdW1\":[..],\"Vdb1\":[..],\"VdW2\":[..],\"Vdb2\":[..],...}\n",
    "                    Gradient-Squares parameters, {\"GdW1\":[..],\"Gdb1\":[..],\"GdW2\":[..],\"Gdb2\":[..],...}\n",
    "                    Note: It is just one dictionary (parameters) with all these key value pairs, not multiple dictionaries\n",
    "    '''\n",
    "    \n",
    "    L = parameters['numLayers'] \n",
    "    parameters['apply_momentum'] = apply_momentum\n",
    "    \n",
    "    # Initialize Velocity and the Gradient-Squares to zeros the same size as the corresponding parameters W's abd b's\n",
    "    for l in range(L):\n",
    "        if apply_momentum:\n",
    "            # Hint: Velocity parameters are represented as VdW and Vdb\n",
    "            #      Gradient-Squares are represented as GdW and Gdb\n",
    "            \n",
    "            parameters[\"VdW\" + str(l+1)] =  np.zeros((parameters[\"W\"+str(l+1)].shape))\n",
    "            parameters[\"Vdb\" + str(l+1)] = np.zeros((parameters[\"b\"+str(l+1)].shape))\n",
    "            parameters[\"GdW\" + str(l+1)] = np.zeros((parameters[\"W\"+str(l+1)].shape))\n",
    "            parameters[\"Gdb\" + str(l+1)] = np.zeros((parameters[\"b\"+str(l+1)].shape))\n",
    "            \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "372ead70fa94bbbb5eefe274e3222488",
     "grade": false,
     "grade_id": "cell-e8ad576b87cea2fb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Batchnorm Parameters and corresponding Velocity and Gradient-Squares Initialization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a6f2e5619b0754c69df03c22395830cd",
     "grade": false,
     "grade_id": "cell-74f99853a41130d6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def initialize_bnorm_params(parameters, bnorm_list, apply_momentum):\n",
    "    '''\n",
    "    The function will add batchnorm parameters beta's and gamma's and their corresponding\n",
    "    Velocity and Gradient-Squares to the parameters dictionary\n",
    "    \n",
    "    Inputs: \n",
    "        parameters: dictionary that contains,\n",
    "                    {\"numLayers\":..}\n",
    "                    activations, {\"act1\":\"..\", \"act2\":\"..\", ...}\n",
    "                    dropouts, {\"dropout1\": .. , \"dropout2\": .., ...}\n",
    "                    {\"apply_momentum\":..}\n",
    "                    velocity parameters, {\"VdW1\":[..],\"Vdb1\":[..],\"VdW2\":[..],\"Vdb2\":[..],...}\n",
    "                    Gradient-Squares parameters, {\"GdW1\":[..],\"Gdb1\":[..],\"GdW2\":[..],\"Gdb2\":[..],...}\n",
    "                    Note: It is just one dictionary (parameters) with all these key value pairs, not multiple dictionaries\n",
    "        bnorm_list: binary list indicating if batchnorm should be implemented for a layer\n",
    "        apply_momentum: boolean on whether to apply momentum\n",
    "        \n",
    "    Outputs:\n",
    "        parameters: dictionary that has been updated to include batchnorm parameters, beta, gamma \n",
    "                    and their corresponding momentum parameters. It now contains,\n",
    "                    {\"numLayers\":..}\n",
    "                    activations, {\"act1\":\"..\", \"act2\":\"..\", ...}\n",
    "                    dropouts, {\"dropout1\": .. , \"dropout2\": .., ...}\n",
    "                    velocity parameters, {\"VdW1\":[..],\"Vdb1\":[..],\"VdW2\":[..],\"Vdb2\":[..],...}\n",
    "                    Gradient-Squares parameters, {\"GdW1\":[..],\"Gdb1\":[..],\"GdW2\":[..],\"Gdb2\":[..],...}\n",
    "                    {\"bnorm_list\":..}\n",
    "                    batchnorm parameters, {\"bnorm_beta1\":[..],\"bnorm_gamma1\":[..],\"bnorm_beta2\":[..],\"bnorm_gamma2\":[..],...}\n",
    "                    batchnorm velocity parameters, {\"Vbnorm_beta1\":[..],\"Vbnorm_gamma1\":[..],\"Vbnorm_beta2\":[..],\"Vbnorm_gamma2\":[..],...}\n",
    "                    batchnorm Gradient-Square parameters, {\"Gbnorm_beta1\":[..],\"Gbnorm_gamma1\":[..],\"Gbnorm_beta2\":[..],\"Gbnorm_gamma2\":[..],...}\n",
    "                    Note: It is just one dictionary (parameters) with all these key value pairs, not multiple dictionaries\n",
    "    '''\n",
    "    \n",
    "    L = parameters['numLayers']\n",
    "    parameters['bnorm_list'] = bnorm_list\n",
    "    \n",
    "    # Initialize batchnorm parameters for the hidden layers only. \n",
    "    # Each hidden layer will have a dictionary of parameters, beta and gamma based on the dimensions of the hidden layer. \n",
    "    for l in range(L):\n",
    "        if bnorm_list[l]:\n",
    "            n = parameters[\"W\" + str(l+1)].shape[0]\n",
    "            parameters['bnorm_beta'+str(l+1)] = np.random.randn(n,1)\n",
    "            parameters['bnorm_gamma'+str(l+1)] = np.random.randn(n,1)\n",
    "            if apply_momentum:\n",
    "                parameters['Vbnorm_beta'+str(l+1)] = np.zeros((n,1))\n",
    "                parameters['Gbnorm_beta'+str(l+1)] = np.zeros((n,1))\n",
    "                parameters['Vbnorm_gamma'+str(l+1)] = np.zeros((n,1))\n",
    "                parameters['Gbnorm_gamma'+str(l+1)] = np.zeros((n,1))\n",
    "        else:\n",
    "            parameters['bnorm_beta'+str(l+1)] = np.asarray([])\n",
    "            parameters['Vbnorm_beta'+str(l+1)] = np.asarray([])\n",
    "            parameters['Gbnorm_beta'+str(l+1)] = np.asarray([])\n",
    "            parameters['bnorm_gamma'+str(l+1)] = np.asarray([])\n",
    "            parameters['Vbnorm_gamma'+str(l+1)] = np.asarray([])\n",
    "            parameters['Gbnorm_gamma'+str(l+1)] = np.asarray([])\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ba2dbfdf3f3b183229914f723e1417d7",
     "grade": false,
     "grade_id": "cell-36236b2868ecda30",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Forward Propagation Through a Single Layer\n",
    "\n",
    "If the vectorized input to any layer of neural network is $A\\_prev$ and the parameters of the layer are given by $(W,b)$, the output of the layer (before the activation is):\n",
    "\\begin{equation}\n",
    "Z = W.A\\_prev + b\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ed4865aef2f388d832326ef6c4160c95",
     "grade": false,
     "grade_id": "forward_single_layer_soln",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def linear_forward(A_prev, W, b):\n",
    "    '''\n",
    "    Input A_prev propagates through the layer \n",
    "    Z = WA + b is the output of this layer. \n",
    "\n",
    "    Inputs: \n",
    "        A_prev: numpy.ndarray (n,m) the input to the layer\n",
    "        W: numpy.ndarray (n_out, n) the weights of the layer\n",
    "        b: numpy.ndarray (n_out, 1) the bias of the layer\n",
    "\n",
    "    Outputs:\n",
    "        Z: where Z = W.A_prev + b, where Z is the numpy.ndarray (n_out, m) dimensions\n",
    "        cache: a dictionary containing the inputs A\n",
    "    '''\n",
    "    Z = np.dot(W,A_prev) + b \n",
    "\n",
    "    cache = {}\n",
    "    cache[\"A\"] = A_prev\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a78556ac91473852bcf96924e78854ad",
     "grade": false,
     "grade_id": "cell-3c463f4362ff9844",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Forward Propagation Through a Layer (linear $\\rightarrow$ activation $\\rightarrow$ batchnorm $\\rightarrow$ dropout)\n",
    "\n",
    "The input to the layer propagates through the layer in the order linear $\\rightarrow$ activation $\\rightarrow$ batchnorm $\\rightarrow$ dropout saving different cache along the way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "07631dfdb8eefbc842508a3536dffaaa",
     "grade": false,
     "grade_id": "cell-f70a31ac49d9bb4c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def layer_forward(A_prev, W, b, activation, drop_prob, bnorm_beta, bnorm_gamma, mode):\n",
    "    '''\n",
    "    Input A_prev propagates through the layer followed by activation, batchnorm and dropout\n",
    "\n",
    "    Inputs: \n",
    "        A_prev: numpy.ndarray (n,m) the input to the layer\n",
    "        W: numpy.ndarray (n_out, n) the weights of the layer\n",
    "        b: numpy.ndarray (n_out, 1) the bias of the layer\n",
    "        activation: is the string that specifies the activation function\n",
    "        drop_prob: dropout parameter. If drop_prob = 0.3, we drop 30% of the neuron activations\n",
    "        bnorm_beta: batchnorm beta \n",
    "        bnorm_gamma: batchnorm gamma\n",
    "        mode: 'train' or 'test' Dropout acts differently in training and testing mode. Hence, mode is a parameter which\n",
    "                takes in only 2 values, 'train' or 'test'\n",
    "\n",
    "    Outputs:\n",
    "        A: = g(Z), where Z = WA + b, where Z is the numpy.ndarray (n_out, m) dimensions\n",
    "        g is the activation function\n",
    "        cache: a dictionary containing the cache from the linear propagation, activation, bacthnorm and dropout\n",
    "        to be used for derivative\n",
    "    '''\n",
    "    \n",
    "    Z, lin_cache = linear_forward(A_prev, W, b)\n",
    "    if activation == \"relu\":\n",
    "        A, act_cache = relu(Z)\n",
    "    elif activation == \"linear\":\n",
    "        A, act_cache = linear(Z)\n",
    "    \n",
    "    A, bnorm_cache = batchnorm(A, bnorm_beta, bnorm_gamma)\n",
    "    A, drop_cache = dropout(A, drop_prob, mode)\n",
    "    cache = {}\n",
    "    cache[\"lin_cache\"] = lin_cache\n",
    "    cache[\"act_cache\"] = act_cache\n",
    "    cache[\"bnorm_cache\"] = bnorm_cache\n",
    "    cache[\"drop_cache\"] = drop_cache\n",
    "    \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "611df0adfac42ead393f088d11e8826e",
     "grade": false,
     "grade_id": "cell-243ec2c02ff1834c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Multi-Layers Forward Propagation\n",
    "\n",
    "Starting with the input 'A0' and the first layer of the network, we will propgate A0 through every layer using the output of the previous layer as input to the next layer. we will gather the caches from every layer in a list and use it later for backpropagation. We will use the 'layer_forward(.)' function to get the output and caches for a layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dd87521073a2f0a0c07338d938566eb7",
     "grade": false,
     "grade_id": "cell-40ada61852849730",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def multi_layer_forward(A0, parameters, mode):\n",
    "    '''\n",
    "    Forward propgation through the layers of the network\n",
    "\n",
    "    Inputs: \n",
    "        A0: numpy.ndarray (n,m) with n features and m samples\n",
    "        parameters: dictionary of network parameters {\"W1\":[..],\"b1\":[..],\"W2\":[..],\"b2\":[..]...}\n",
    "        mode: 'train' or 'test' Dropout acts differently in training and testing mode. Hence, mode is a parameter which\n",
    "                takes in only 2 values, 'train' or 'test' \n",
    "    \n",
    "    Outputs:\n",
    "        AL: numpy.ndarray (c,m)  - outputs of the last fully connected layer before softmax\n",
    "            where c is number of categories and m is number of samples\n",
    "        caches: a list of caches from every layer after forward propagation\n",
    "    '''\n",
    "    \n",
    "    L = parameters['numLayers']\n",
    "    A = A0\n",
    "    caches = []\n",
    "    for l in range(L):\n",
    "        A, cache = layer_forward(A, parameters[\"W\"+str(l+1)], parameters[\"b\"+str(l+1)], \\\n",
    "                                 parameters[\"act\"+str(l+1)], parameters[\"dropout\"+str(l+1)], \\\n",
    "                                 parameters['bnorm_beta'+str(l+1)], parameters['bnorm_gamma'+str(l+1)], mode)\n",
    "        caches.append(cache)\n",
    "    return A, caches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "136f3914445b836e7df4bba3238ce216",
     "grade": false,
     "grade_id": "cell-5281900868c84d39",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Backward Propagagtion for the Linear Computation of a Layer\n",
    "\n",
    "Consider the linear layer $Z = W.A\\_prev + b$. We would like to estimate the gradients $\\frac{dL}{dW}$ - represented as $dW$, $\\frac{dL}{db}$ - represented as $db$ and $\\frac{dL}{dA\\_prev}$ - represented as $dA\\_prev$. \n",
    "The input to estimate these derivatives is $\\frac{dL}{dZ}$ - represented as $dZ$. The derivatives are given by, \n",
    "\n",
    "\\begin{equation}\n",
    "dA\\_prev = W^T dZ\\\\\n",
    "dW = dZ A^T\\\\\n",
    "db = \\sum_{i=1}^{m} dZ^{(i)}\\\\\n",
    "\\end{equation}\n",
    "\n",
    "where $dZ = [dz^{(1)},dz^{(2)}, \\ldots, dz^{(m)}]$ is $(n \\times m)$ matrix of derivatives. \n",
    "The figure below represents a case fo binary cassification where $dZ$ is of dimensions $(1 \\times m)$. The example can be extended to $(n\\times m)$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cf41a82263726e6035c2585266317b26",
     "grade": false,
     "grade_id": "linear_backward_soln",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache, W, b):\n",
    "    '''\n",
    "    Backward prpagation through the linear layer\n",
    "\n",
    "    Inputs:\n",
    "        dZ: numpy.ndarray (n,m) derivative dL/dz \n",
    "        cache: a dictionary containing the inputs A, for the linear layer\n",
    "            where Z = WA + b,    \n",
    "            Z is (n,m); W is (n,p); A is (p,m); b is (n,1)\n",
    "        W: numpy.ndarray (n,p)\n",
    "        b: numpy.ndarray (n,1)\n",
    "\n",
    "    Outputs:\n",
    "        dA_prev: numpy.ndarray (p,m) the derivative to the previous layer\n",
    "        dW: numpy.ndarray (n,p) the gradient of W \n",
    "        db: numpy.ndarray (n,1) the gradient of b\n",
    "    '''\n",
    "    \n",
    "    A = cache[\"A\"]\n",
    "    A = np.array(A)\n",
    "    dW = np.dot(dZ, A.T)\n",
    "    db = np.sum(dZ,axis=1, keepdims=True)\n",
    "    dA_prev = np.dot(W.T,dZ)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2e7ae2a750bd9690d211025d37278304",
     "grade": false,
     "grade_id": "cell-f57dc4108dd56c38",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Back Propagation Through a Layer (dropout $\\rightarrow$ batchnorm $\\rightarrow$ activation $\\rightarrow$ linear)\n",
    "\n",
    "We will define the backpropagation for a layer. We will use the backpropagation for the dropout, followed by backpropagation for batchnorm, backpropagation of activation and backpropagation of a linear layer, in that order. This is the reverse order to the forward propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "21b514c5a427c236a4954e4465ad2fc4",
     "grade": false,
     "grade_id": "cell-9b7cfa89255f3e03",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def layer_backward(dA, cache, W, b, activation):\n",
    "    '''\n",
    "    Backward propagation through the activation and linear layer\n",
    "\n",
    "    Inputs:\n",
    "        dA: numpy.ndarray (n,m) the derivative to the previous layer\n",
    "        cache: dictionary containing the linear_cache and the activation_cache\n",
    "        W: numpy.ndarray (n,p)\n",
    "        b: numpy.ndarray (n,1)\n",
    "        activation: activation of the layer, 'relu' or 'linear'\n",
    "    \n",
    "    Outputs:\n",
    "        dA_prev: numpy.ndarray (p,m) the derivative to the previous layer\n",
    "        dW: numpy.ndarray (n,p) the gradient of W \n",
    "        db: numpy.ndarray (n,1) the gradient of b\n",
    "        dbnorm_beta: numpy.ndarray (n,1) derivative of beta for the batchnorm layer\n",
    "        dbnorm_gamma: numpy.ndarray (n,1) derivative of gamma for the batchnorm layer\n",
    "    '''\n",
    "\n",
    "    lin_cache = cache[\"lin_cache\"]\n",
    "    act_cache = cache[\"act_cache\"]\n",
    "    drop_cache = cache[\"drop_cache\"]\n",
    "    bnorm_cache = cache[\"bnorm_cache\"]\n",
    "    \n",
    "    dA = dropout_der(dA, drop_cache)\n",
    "    dA, dbnorm_beta, dbnorm_gamma = batchnorm_der(dA, cache[\"bnorm_cache\"])\n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_der(dA, act_cache)\n",
    "    elif activation == \"linear\":\n",
    "        dZ = linear_der(dA, act_cache)\n",
    "        \n",
    "    dA_prev, dW, db = linear_backward(dZ, lin_cache, W, b)\n",
    "    return dA_prev, dW, db, dbnorm_beta, dbnorm_gamma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "63dd4a8b2ce39e28960758bcc6393bfc",
     "grade": false,
     "grade_id": "cell-72a9dc0cb265dc90",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Multi-layers Back Propagation\n",
    "\n",
    "We have defined the required functions to handle back propagation for a single layer. Now we will stack the layers together and perform back propagation on the entire network starting with the final layer. We will need teh caches stored during forward propagation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b3c530dd1f62e5cee0dc1c3626ab8832",
     "grade": false,
     "grade_id": "cell-8d2141e7c67dafa5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def multi_layer_backward(dAL, caches, parameters):\n",
    "    '''\n",
    "    Back propgation through the layers of the network (except softmax cross entropy)\n",
    "    softmax_cross_entropy can be handled separately\n",
    "\n",
    "    Inputs: \n",
    "        dAL: numpy.ndarray (n,m) derivatives from the softmax_cross_entropy layer\n",
    "        caches: a dictionary of associated caches of parameters and network inputs\n",
    "        parameters: dictionary of network parameters {\"W1\":[..],\"b1\":[..],\"W2\":[..],\"b2\":[..]...}\n",
    "\n",
    "    Outputs:\n",
    "        gradients: dictionary of gradient of network parameters \n",
    "            {\"dW1\":[..],\"db1\":[..],\"dW2\":[..],\"db2\":[..],...\\\n",
    "            \"dbnorm_beta1\":[..],\"dbnorm_gamma1\":[..],\"dbnorm_beta2\":[..],\"dbnorm_gamma2\":[..],...}\n",
    "    '''\n",
    "\n",
    "    L = len(caches) \n",
    "    gradients = {}\n",
    "    dA = dAL\n",
    "    activation = \"linear\"\n",
    "    for l in reversed(range(L)):\n",
    "        dA, gradients[\"dW\"+str(l+1)], gradients[\"db\"+str(l+1)], \\\n",
    "        gradients[\"dbnorm_beta\"+str(l+1)], gradients[\"dbnorm_gamma\"+str(l+1)] \\\n",
    "                    = layer_backward(dA, caches[l], parameters[\"W\"+str(l+1)],\\\n",
    "                                     parameters[\"b\"+str(l+1)],parameters[\"act\"+str(l+1)])\n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b6f9f7593d557173167d6a84f6f678cb",
     "grade": false,
     "grade_id": "cell-4cb88cabf2344894",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### STEP5. Parameter Update Using Adam (momentum)\n",
    "\n",
    "The parameter gradients $(dW,db)$ calculated during back propagation are used to update the values of the network parameters using Adam optmization which is the momentum technique we discussed in the lecture.\n",
    "\n",
    "\\begin{equation} \n",
    "V_{t+1} = \\beta V_{t} +(1-\\beta)\\nabla J(\\theta_t)\\\\ \n",
    "G_{t+1} = \\beta_2 G_{t} +(1-\\beta_2)\\nabla J(\\theta_t)^2\\\\ \n",
    "\\theta_{t+1} =\\theta_{t} -\\frac{\\alpha}{\\sqrt{G_{t+1}+\\epsilon}}V_{t+1}, \\quad \\theta \\in \\{ W,b \\} \n",
    "\\end{equation}\n",
    "\n",
    "Parameters $V$ are the momentum velocity parameters and parameters $G$ are the Gradient-Squares. \n",
    "$\\nabla J(\\theta)$ is the gradient term $dW$ or $db$, and $\\nabla  J(\\theta)^2$ is the element wise square of the gradient. \n",
    "$\\alpha$ is the step_size for gradient descent. It is has been estimated by decaying the learning_rate based on decay_rate and epoch number. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5b17fe2a2e6d731a24e28a6235d1c389",
     "grade": false,
     "grade_id": "update_momentum_soln",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def update_parameters_with_momentum_Adam(parameters, gradients, alpha, beta=0.9, beta2=0.99, eps=1e-8):\n",
    "    '''\n",
    "    Updates the network parameters with gradient descent\n",
    "\n",
    "    Inputs:\n",
    "        parameters: dictionary of \n",
    "                    network parameters, {\"W1\":[..],\"b1\":[..],\"W2\":[..],\"b2\":[..],...}\n",
    "                    velocity parameters, {\"VdW1\":[..],\"Vdb1\":[..],\"VdW2\":[..],\"Vdb2\":[..],...}\n",
    "                    Gradient-Squares parameters, {\"GdW1\":[..],\"Gdb1\":[..],\"GdW2\":[..],\"Gdb2\":[..],...}\n",
    "                    batchnorm parameters, {\"bnorm_beta1\":[..],\"bnorm_gamma1\":[..],\"bnorm_beta2\":[..],\"bnorm_gamma2\":[..],...}\n",
    "                    batchnorm velocity parameters, {\"Vbnorm_beta1\":[..],\"Vbnorm_gamma1\":[..],\"Vbnorm_beta2\":[..],\"Vbnorm_gamma2\":[..],...}\n",
    "                    batchnorm Gradient-Square parameters, {\"Gbnorm_beta1\":[..],\"Gbnorm_gamma1\":[..],\"Gbnorm_beta2\":[..],\"Gbnorm_gamma2\":[..],...}\n",
    "                    and other parameters \n",
    "                    :\n",
    "                    :\n",
    "                    Note: It is just one dictionary (parameters) with all these key value pairs, not multiple dictionaries\n",
    "        gradients: dictionary of gradient of network parameters \n",
    "                   {\"dW1\":[..],\"db1\":[..],\"dW2\":[..],\"db2\":[..],...}\n",
    "        alpha: stepsize for the gradient descent\n",
    "        beta: beta parameter for momentum (same as beta1 in Adam)\n",
    "        beta2: beta2 parameter for Adam\n",
    "        eps: epsilon parameter for Adam\n",
    "        \n",
    "    Outputs: \n",
    "        parameters: updated dictionary of \n",
    "                    network parameters, {\"W1\":[..],\"b1\":[..],\"W2\":[..],\"b2\":[..],...}\n",
    "                    velocity parameters, {\"VdW1\":[..],\"Vdb1\":[..],\"VdW2\":[..],\"Vdb2\":[..],...}\n",
    "                    Gradient-Squares parameters, {\"GdW1\":[..],\"Gdb1\":[..],\"GdW2\":[..],\"Gdb2\":[..],...}\n",
    "                    batchnorm parameters, {\"bnorm_beta1\":[..],\"bnorm_gamma1\":[..],\"bnorm_beta2\":[..],\"bnorm_gamma2\":[..],...}\n",
    "                    batchnorm velocity parameters, {\"Vbnorm_beta1\":[..],\"Vbnorm_gamma1\":[..],\"Vbnorm_beta2\":[..],\"Vbnorm_gamma2\":[..],...}\n",
    "                    batchnorm Gradient-Square parameters, {\"Gbnorm_beta1\":[..],\"Gbnorm_gamma1\":[..],\"Gbnorm_beta2\":[..],\"Gbnorm_gamma2\":[..],...}\n",
    "                    and other parameters \n",
    "                    :\n",
    "                    :\n",
    "                    Note: It is just one dictionary (parameters) with all these key value pairs, not multiple dictionaries\n",
    "             \n",
    "    '''\n",
    "    L = parameters['numLayers']\n",
    "    apply_momentum = parameters['apply_momentum']\n",
    "    bnorm_list = parameters['bnorm_list']\n",
    "    \n",
    "    for l in range(L):\n",
    "        if apply_momentum:\n",
    "            # Apply Adam momentum to parameters W's and b's. \n",
    "            # You will need to update the Velocity parameters VdW's and Vdb's\n",
    "            parameters[\"VdW\" + str(l+1)] = beta * parameters[\"VdW\" + str(l+1)] + (1 - beta) * gradients[\"dW\" + str(l+1)]\n",
    "            parameters[\"Vdb\" + str(l+1)] = beta * parameters[\"Vdb\" + str(l+1)] + (1 - beta) * gradients[\"db\" + str(l+1)]\n",
    "            # You will need to update the Gradient-Squares parameters GdW's and Gdb's\n",
    "            parameters[\"GdW\" + str(l+1)] = beta2 * parameters[\"GdW\" + str(l+1)] + (1 - beta2) * (gradients[\"dW\" + str(l+1)])**2\n",
    "            parameters[\"Gdb\" + str(l+1)] = beta2 * parameters[\"Gdb\" + str(l+1)] + (1 - beta2) * (gradients[\"db\" + str(l+1)])**2\n",
    "            # You will need to update the parameters W's and b's\n",
    "            parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - alpha/((parameters[\"GdW\" + str(l+1)]+eps)**0.5)*parameters[\"VdW\" + str(l+1)]\n",
    "            parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - alpha/((parameters[\"Gdb\" + str(l+1)]+eps)**0.5)*parameters[\"Vdb\" + str(l+1)]\n",
    "            \n",
    "        else:\n",
    "            # When no momentum is required apply regular gradient descent\n",
    "            parameters[\"W\"+str(l+1)] -= alpha * gradients[\"dW\"+str(l+1)]\n",
    "            parameters[\"b\"+str(l+1)] -= alpha * gradients[\"db\"+str(l+1)]\n",
    "        \n",
    "        # The Adam momentum for batch norm parameters has been implemented below\n",
    "        if apply_momentum and bnorm_list[l]:\n",
    "            parameters['Vbnorm_beta'+str(l+1)] = beta*parameters['Vbnorm_beta'+str(l+1)] + \\\n",
    "                                                    (1 - beta)*gradients[\"dbnorm_beta\"+str(l+1)]\n",
    "            parameters['Vbnorm_gamma'+str(l+1)] = beta*parameters['Vbnorm_gamma'+str(l+1)] + \\\n",
    "                                                    (1 - beta)*gradients[\"dbnorm_gamma\"+str(l+1)]\n",
    "            parameters['Gbnorm_beta'+str(l+1)] = beta2*parameters['Gbnorm_beta'+str(l+1)] + \\\n",
    "                                                    (1 - beta2)*(gradients[\"dbnorm_beta\"+str(l+1)]**2)\n",
    "            parameters['Gbnorm_gamma'+str(l+1)] = beta2*parameters['Gbnorm_gamma'+str(l+1)] + \\\n",
    "                                                    (1 - beta2)*(gradients[\"dbnorm_gamma\"+str(l+1)]**2)\n",
    "            parameters['bnorm_beta' + str(l+1)] = parameters['bnorm_beta' + str(l+1)] \\\n",
    "                        - alpha*parameters['Vbnorm_beta'+str(l+1)]/np.sqrt(parameters['Gbnorm_beta'+str(l+1)] + eps)\n",
    "            parameters['bnorm_gamma' + str(l+1)] = parameters['bnorm_gamma' + str(l+1)] \\\n",
    "                        - alpha*parameters['Vbnorm_gamma'+str(l+1)]/np.sqrt(parameters['Gbnorm_gamma'+str(l+1)] + eps)\n",
    "        elif bnorm_list[l]:\n",
    "            parameters['bnorm_beta' + str(l+1)] -= alpha * gradients[\"dbnorm_beta\"+str(l+1)]\n",
    "            parameters['bnorm_gamma' + str(l+1)] -= alpha * gradients[\"dbnorm_beta\"+str(l+1)]\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dd7d5ca20b9bd5d6aa52f678918cd624",
     "grade": false,
     "grade_id": "cell-76abe4d415a1f55e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### STEP6. Multilayer Neural Network\n",
    "\n",
    "Let us now assemble all the components of the neural network together and define a complete training loop for a Multi-layer Neural Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e0f5f0363eb77f5dd63edc993f5e1608",
     "grade": false,
     "grade_id": "multi_layer_network_soln",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def multi_layer_network(X, Y, net_dims, act_list, drop_prob_list, bnorm_list, num_epochs=3, \n",
    "                        batch_size=64, learning_rate=0.2, decay_rate=0.01, apply_momentum=True, log=True, log_step=200):\n",
    "    \n",
    "    '''\n",
    "    Creates the multilayer network and trains the network\n",
    "\n",
    "    Inputs:\n",
    "        X: numpy.ndarray (n,m) of training data\n",
    "        Y: numpy.ndarray (1,m) of training data labels\n",
    "        net_dims: tuple of layer dimensions\n",
    "        act_list: list of strings indicating the activations for each layer\n",
    "        drop_prob_list: list of dropout probabilities for each layer \n",
    "        bnorm_list: binary list indicating presence or absence of batchnorm for each layer\n",
    "        num_epochs: num of epochs to train\n",
    "        batch_size: batch size for training\n",
    "        learning_rate: learning rate for gradient descent\n",
    "        decay_rate: rate of learning rate decay\n",
    "        apply_momentum: boolean whether to apply momentum or not\n",
    "        log: boolean whether to print training progression \n",
    "        log_step: prints training progress every log_step iterations\n",
    "    \n",
    "    Outputs:\n",
    "        costs: list of costs (or loss) over training\n",
    "        parameters: dictionary of \n",
    "                    network parameters, {\"W1\":[..],\"b1\":[..],\"W2\":[..],\"b2\":[..],...}\n",
    "                    velocity parameters, {\"VdW1\":[..],\"Vdb1\":[..],\"VdW2\":[..],\"Vdb2\":[..],...}\n",
    "                    Gradient-Squares parameters, {\"GdW1\":[..],\"Gdb1\":[..],\"GdW2\":[..],\"Gdb2\":[..],...}\n",
    "                    batchnorm parameters, {\"bnorm_beta1\":[..],\"bnorm_gamma1\":[..],\"bnorm_beta2\":[..],\"bnorm_gamma2\":[..],...}\n",
    "                    batchnorm velocity parameters, {\"Vbnorm_beta1\":[..],\"Vbnorm_gamma1\":[..],\"Vbnorm_beta2\":[..],\"Vbnorm_gamma2\":[..],...}\n",
    "                    batchnorm Gradient-Square parameters, {\"Gbnorm_beta1\":[..],\"Gbnorm_gamma1\":[..],\"Gbnorm_beta2\":[..],\"Gbnorm_gamma2\":[..],...}\n",
    "                    Note: It is just one dictionary (parameters) with all these key value pairs, not multiple dictionaries\n",
    "    '''\n",
    "    mode = 'train'\n",
    "    n, m = X.shape\n",
    "    parameters = initialize_network(net_dims, act_list, drop_prob_list)\n",
    "    parameters = initialize_velocity(parameters, apply_momentum)\n",
    "    parameters = initialize_bnorm_params(parameters, bnorm_list, apply_momentum)\n",
    "    costs = []\n",
    "    itr = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        # estimate stepsize alpha using decay_rate on learning rate using epoch number\n",
    "        alpha = learning_rate*(1/(1+decay_rate*epoch))\n",
    "        if log:\n",
    "            print('------- Epoch {} -------'.format(epoch+1))\n",
    "        for ii in range((m - 1) // batch_size + 1):\n",
    "            Xb = X[:, ii*batch_size : (ii+1)*batch_size]\n",
    "            Yb = Y[:, ii*batch_size : (ii+1)*batch_size]\n",
    "            A0 = Xb\n",
    "        \n",
    "            ## Forward Propagation\n",
    "            # Step 1: Input 'A0', 'parameters' and 'mode' into the network \n",
    "            #         using multi_layer_forward() and calculate output of last layer 'A' (before softmax) \n",
    "            #         and obtain cached activations as 'caches'\n",
    "            A, caches = multi_layer_forward(A0, parameters,mode)\n",
    "            # Step 2: Input 'A' and groundtruth labels 'Yb' to softmax_cros_entropy_loss(.) and estimate\n",
    "            #         activations 'AL', 'softmax_cache' and 'cost'\n",
    "            AL, softmax_cache, cost = softmax_cross_entropy_loss(A,Yb)\n",
    "            ## Back Propagation\n",
    "            # Step 3: Estimate gradient 'dAL' with softmax_cros_entropy_loss_der(.) using groundtruth \n",
    "            #         labels 'Yb' and 'softmax_cache' \n",
    "            dAL = softmax_cross_entropy_loss_der(Yb, softmax_cache)\n",
    "            # Step 4: Estimate 'gradients' with multi_layer_backward(.) using 'dAL', 'caches' and 'parameters' \n",
    "            gradients = multi_layer_backward(dAL, caches, parameters)\n",
    "            # Step 5: Estimate updated 'parameters' with update_parameters_with_momentum_Adam(.) \n",
    "            #         using 'parameters', 'gradients' and 'alpha'\n",
    "            #         Note: Use the same variable 'parameters' as input and output to the update_parameters(.) function\n",
    "            parameters = update_parameters_with_momentum_Adam(parameters, gradients, alpha)\n",
    "            \n",
    "\n",
    "            if itr % log_step == 0:\n",
    "                costs.append(cost)\n",
    "                if log:\n",
    "                    print(\"Cost at iteration %i is: %.05f, learning rate: %.05f\" %(itr, cost, alpha))\n",
    "            itr+=1\n",
    "    \n",
    "    return costs, parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8258b52466398b97d07c4fac54a14329",
     "grade": false,
     "grade_id": "cell-66defde04ecba045",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### STEP7. Prediction\n",
    "\n",
    "This is the evaluation function which will predict the labels for a minibatch of inputs samples\n",
    "We will perform forward propagation through the entire network and determine the class predictions for the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c648f2b41d8b9602fac153cb151e1127",
     "grade": false,
     "grade_id": "classify_soln",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def classify(X, parameters, mode='test'):\n",
    "    '''\n",
    "    Network prediction for inputs X\n",
    "\n",
    "    Inputs: \n",
    "        X: numpy.ndarray (n,m) with n features and m samples\n",
    "        parameters: dictionary of network parameters \n",
    "            {\"W1\":[..],\"b1\":[..],\"W2\":[..],\"b2\":[..],...}\n",
    "        drop_prob_list: list of dropout probabilities for each layer \n",
    "        mode: 'train' or 'test' Dropout acts differently in training and testing mode.\n",
    "        \n",
    "    Outputs:\n",
    "        YPred: numpy.ndarray (1,m) of predictions\n",
    "    '''\n",
    "    # Using multi_layer_forward(.) Forward propagate input 'X' with 'parameters' and mode to \n",
    "    #        obtain the final activation 'A'\n",
    "    A, caches = multi_layer_forward(X,parameters,mode)\n",
    "    # Using 'softmax_cross_entropy loss(.)', obtain softmax activation 'AL' with input 'A' from step 1\n",
    "    AL, softmax_cache, cost = softmax_cross_entropy_loss(A)\n",
    "    # Estimate 'YPred' as the 'argmax' of softmax activation from step-2. These are the label predictions \n",
    "    \n",
    "    # Note: the shape of 'YPred' should be (1,m), where m is the number of samples\n",
    "        \n",
    "    YPred = np.argmax(AL,axis = 0)\n",
    "    YPred =np.reshape(YPred, (1,YPred.size))\n",
    "    \n",
    "    return YPred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0a8d461d5399d9e219915a9fb4744e46",
     "grade": false,
     "grade_id": "cell-f44aae42add8fd84",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### STEP8. Training\n",
    "\n",
    "We will now intialize a neural network with 3 hidden layers whose dimensions are 100, 100 and 64. \n",
    "Since the input samples are of dimension 28 $\\times$ 28, the input layer will be of dimension 784. The output dimension is 10 since we have a 10 category classification. \n",
    "We will train the model and compute its accuracy on both training and test sets and plot the training cost (or loss) against the number of iterations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e838550fae62d69a1a9965e4800dfd35",
     "grade": false,
     "grade_id": "test_acc_soln",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# You should be able to get a train accuracy of >90% and a test accuracy >85% \n",
    "# The settings below gave >95% train accuracy and >90% test accuracy \n",
    "\n",
    "# Feel free to adjust the values and explore how the network behaves\n",
    "net_dims = [784, 100, 100, 64, 10]\n",
    "#This network has 4 layers\n",
    "#784 is for image dimensions\n",
    "#10 is for number of categories \n",
    "#100 and 64 are arbitrary\n",
    "\n",
    "# list of dropout probabilities for each layer\n",
    "# The length of the list is equal to the number of layers\n",
    "# Note: Has to be same length as net_dims. 0 indicates no dropout\n",
    "drop_prob_list = [0, 0, 0, 0]\n",
    "\n",
    "# binary list indicating if batchnorm should be implemented for a layer\n",
    "# The length of the list is equal to the number of layers\n",
    "# 1 indicates bathnorm and 0 indicates no batchnorm\n",
    "# If your implementation of batchnorm is incorrect, then set bnorm_list = [0,0,0,0]\n",
    "bnorm_list = [1,1,1,1]\n",
    "assert(len(bnorm_list) == len(net_dims)-1)\n",
    "\n",
    "# list of strings indicating the activation for a layer\n",
    "# The length of the list is equal to the number of layers\n",
    "# The last layer is usually a linear before the softmax\n",
    "act_list = ['relu', 'relu', 'relu', 'linear']\n",
    "assert(len(act_list) == len(net_dims)-1)\n",
    "    \n",
    "# initialize learning rate, decay_rate and num_iterations \n",
    "num_epochs = 3\n",
    "batch_size = 64\n",
    "learning_rate = 1e-2\n",
    "decay_rate = 1\n",
    "apply_momentum = True\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "print(\"Network dimensions are:\" + str(net_dims))\n",
    "print('Dropout= [{}], Batch Size = {}, lr = {}, decay rate = {}'\\\n",
    "      .format(drop_prob_list,batch_size,learning_rate,decay_rate)) \n",
    "\n",
    "# getting the subset dataset from MNIST\n",
    "trX, trY, tsX, tsY = get_mnist()\n",
    "# We need to reshape the data everytime to match the format (d,m), where d is dimensions (784) and m is number of samples\n",
    "trX = trX.reshape(-1, 28*28).T\n",
    "trY = trY.reshape(1, -1)\n",
    "tsX = tsX.reshape(-1, 28*28).T\n",
    "tsY = tsY.reshape(1, -1)\n",
    "\n",
    "costs, parameters = multi_layer_network(trX, trY, net_dims, act_list, drop_prob_list, bnorm_list, \\\n",
    "                                        num_epochs=num_epochs, batch_size=batch_size, learning_rate=learning_rate, \\\n",
    "                                        decay_rate=decay_rate, apply_momentum=apply_momentum, log=True)\n",
    "\n",
    "# compute the accuracy for training set and testing set\n",
    "train_Pred = classify(trX, parameters)\n",
    "\n",
    "test_Pred = classify(tsX, parameters)\n",
    "\n",
    "# Estimate the training accuracy 'trAcc' comparing train_Pred and trY \n",
    "# Estimate the testing accuracy 'teAcc' comparing test_Pred and tsY\n",
    "# your code here\n",
    "\n",
    "\n",
    "print(\"Accuracy for training set is {0:0.3f} %\".format(trAcc))\n",
    "print(\"Accuracy for testing set is {0:0.3f} %\".format(teAcc))\n",
    "\n",
    "plt.plot(range(len(costs)),costs)\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following set up gives an accuracy of > 96% for both test and train. \n",
    "# Feel free to change the settings to get the best accuracy \n",
    "np.random.seed(1)\n",
    "\n",
    "net_dims = [784, 100, 100, 10] \n",
    "drop_prob_list = [0, 0, 0]\n",
    "act_list = ['relu', 'relu', 'linear']\n",
    "    \n",
    "# initialize learning rate, decay_rate and num_iterations \n",
    "num_epochs = 3\n",
    "batch_size = 64\n",
    "learning_rate = 1e-3\n",
    "decay_rate = 0.1\n",
    "apply_momentum = True\n",
    "\n",
    "# If your implementation of batchnorm is incorrect, \n",
    "# then set bnorm_list = [0,0,0] below to run the following testcase without batchnorm. \n",
    "# The test case is still expected to pass without batchnorm when your accuracy is above 95%\n",
    "bnorm_list = [1,1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "64e37b89e48e9733c099ea03a0aedf51",
     "grade": true,
     "grade_id": "test_acc",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# getting the subset dataset from MNIST\n",
    "trX, trY, tsX, tsY = get_mnist()\n",
    "# We need to reshape the data everytime to match the format (d,m), where d is dimensions (784) and m is number of samples\n",
    "trX = trX.reshape(-1, 28*28).T\n",
    "trY = trY.reshape(1, -1)\n",
    "tsX = tsX.reshape(-1, 28*28).T\n",
    "tsY = tsY.reshape(1, -1)\n",
    "\n",
    "costs, parameters = multi_layer_network(trX, trY, net_dims, act_list, drop_prob_list, bnorm_list, \\\n",
    "                                        num_epochs=num_epochs, batch_size=batch_size, learning_rate=learning_rate, \\\n",
    "                                        decay_rate=decay_rate, apply_momentum=apply_momentum, log=False)\n",
    "\n",
    "# compute the accuracy for training set and testing set\n",
    "train_Pred = classify(trX, parameters)\n",
    "test_Pred = classify(tsX, parameters)\n",
    "\n",
    "# Contains hidden tests \n",
    "# Should get atleast 95% train and test accuracy"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
