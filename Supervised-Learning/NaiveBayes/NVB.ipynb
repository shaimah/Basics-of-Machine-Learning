{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Naive Bayes Binary Classifier\n",
    "\n",
    "The Naive Bayes binary classifier is the implementation of a two-class classification algorithm. The code below is tailored for the classification of grayscale images of handwritten digits of 0s and 1s. \n",
    "The MNIST dataset, which is a database of greyscale images of handwritten digits from 0 to 9, is available online. The .mat files of only digits 0s and 1s are uploaded in here for convience.\n",
    "\n",
    "#### STEP1: Loading data\n",
    "\n",
    "\n",
    "The first step is to load the .mat data subsets of 0s and 1s into 3D numpy arrays. The geneNewData.py file will generate a trainsets of 0s and 1s each containing 5000 images each, and testsets of 0s and 1s containing 980 and 1,135 samples, respectively.\n",
    "\n",
    "Given that each image sample is 28 x 28 pixels, the input data structure(2 trainsets and 2 tests) is a 3D array – a collection of 2D image arrays of dimensions n x 28 x 28, where n is the number of samples.\n",
    "\n",
    "The imshow() function in pyplot module of matplotlib library is used to display data as an image. You can use it to display any image in the train and test sets.\n",
    "\n",
    "#### STEP2: Feature extraction\n",
    "\n",
    "Features are simply the independent variables that are chosen carefully to build our model. In case of a grayscale image, eacg pixel value is a single number that represents the brightness of the pixel. The most common pixel format is the byte image, where this number is stored as an 8-bit integer giving a range of possible values from 0 to 255. Typically zero is taken to be black, and 255 is taken to be white.\n",
    "\n",
    "We will be working with a 2D feature vector that comprises the mean and the standard deviation deviation of all the pixel values within an image array.\n",
    "\n",
    "Note that the features should only be extracted from the train datasets\n",
    "\n",
    "#### STEP3: Parameters calculation\n",
    "\n",
    "A supervised learning model is defined by a set of parameters. Based on the 2D datapoints of features generated for the two-class naive bayes classifiers, the following parameters should be calcluated:\n",
    "\n",
    "● (No.1) Mean of feature1 for digit0\n",
    "● (No.2) Variance of feature1 for digit0\n",
    "● (No.3) Mean of feature2 for digit0\n",
    "● (No.4) Variance of feature2 for digit0\n",
    "● (No.5) Mean of feature1 for digit1\n",
    "● (No.6) Variance of feature1 for digit1\n",
    "● (No.7) Mean of feature2 for digit1\n",
    "● (No.8) Variance of feature2 for digit1\n",
    "\n",
    "#### STEP4: Applying the maximum likelihood function\n",
    "\n",
    "The calculated mean and variance capture the data distribution for the training dataset. Now, we will iterate through the test dataset and calculate the mean (feature1) and standard deviation(feature2) of the pixel brightness for each image like we did for the training dataset. This allows us to compute the probability of observing the calculated feature among the normally distributed training dataset for each label. Therefore,we will use the normal distribution equation for each image, we need the following:\n",
    "\n",
    "P(feature1 | y=0)\n",
    "P(feature2 | y=0)\n",
    "P(feature1 | y=1)\n",
    "P(feature2 | y=1)\n",
    "\n",
    "Based on this information, as well the assumption that P(y=0) = P(y=1) = 0.5, we can calculate the posterior probabilities of the labels as follows:\n",
    "\n",
    "P(y=0 | feature1,feature2) = P(feature1 | y=0) * P(feature2 | y=0) * P(y=0)\n",
    "P(y=1 | feature1,feature2) = P(feature1 | y=1) * P(feature2 | y=1) * P(y=1)\n",
    "\n",
    "For each data point, the image is assigned to the label with the greater probability given our 2 selected features.\n",
    "\n",
    "#### STEP5: Evaluating the model\n",
    "\n",
    "The accuracy is simply calculated by dividing the number of right predictions by the total number of images.. enjoy it while it's still simple! :)\n",
    "\n",
    "You may print my_pars as a summary of the parameter values and model accuracies for digits 0 and 1, respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io\n",
    "import math\n",
    "import geneNewData\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "\n",
    "def main():\n",
    "    #STEP1\n",
    "    seed='2006'\n",
    "    geneNewData.geneData(seed)\n",
    "    Numpyfile0 = scipy.io.loadmat('digit0_stu_train'+ seed)\n",
    "    Numpyfile1 = scipy.io.loadmat('digit1_stu_train'+ seed)\n",
    "    Numpyfile2 = scipy.io.loadmat('digit0_testset')\n",
    "    Numpyfile3 = scipy.io.loadmat('digit1_testset')\n",
    "    \n",
    "    \n",
    "    train0 = Numpyfile0.get('target_img')\n",
    "    train1 = Numpyfile1.get('target_img')\n",
    "    test0 = Numpyfile2.get('target_img')\n",
    "    test1 = Numpyfile3.get('target_img')\n",
    "    \n",
    "    print([len(train0),len(train1),len(test0),len(test1)])\n",
    "    print('Your trainset and testset are generated successfully!')\n",
    "    \n",
    "    PIL_matrix = train0[1].reshape((28,28))\n",
    "    plt.imshow(PIL_matrix)\n",
    "    \n",
    "    #STEP2\n",
    "    ##flatten 2D array of each image to 1D, this makes the means and standard deviation calculation much easier\n",
    "    \n",
    "    flat_train0 = []\n",
    "    flat_train1 = []\n",
    "    \n",
    "    for i in range(len(train0)):\n",
    "        x = np.array(train0[i].flatten())\n",
    "        x1 = np.array(train1[i].flatten())\n",
    "        flat_train0 = np.append(flat_train0, x)\n",
    "        flat_train1 = np.append(flat_train1, x1)\n",
    "        \n",
    "    flat_train0 = flat_train0.reshape([5000,784])\n",
    "    flat_train1 = flat_train1.reshape([5000,784])\n",
    "\n",
    "    #Mean and std calculaton for digits 0 and 1 datasets\n",
    "                 \n",
    "    feature1_0 = np.mean(flat_train0, axis = 1)\n",
    "    feature2_0 = np.std(flat_train0, axis = 1)\n",
    "    feature1_1 = np.mean(flat_train1, axis = 1)\n",
    "    feature2_1 = np.std(flat_train1, axis = 1)\n",
    "    \n",
    "    #STEP3\n",
    "    ## Generating a dictionary with all 8 parameters  \n",
    "    my_pars = {}\n",
    "    for i in range(8):\n",
    "        par_name = \"No.%d\" % (i+1)\n",
    "        y = int(i+1)\n",
    "        \n",
    "        if (y < 5):\n",
    "            if (y < 3):\n",
    "                data_col = feature1_0\n",
    "            else:\n",
    "                data_col = feature2_0\n",
    "        else:\n",
    "            if (y < 7):\n",
    "                data_col = feature1_1\n",
    "            else:\n",
    "                data_col = feature2_1\n",
    "        if (y % 2 != 0):\n",
    "            my_pars[par_name] = np.mean(data_col)\n",
    "        else:\n",
    "            my_pars[par_name] = np.var(data_col)\n",
    "\n",
    "    # generating 2D array\n",
    "    flat_test0 = []\n",
    "    flat_test1 = []\n",
    "    \n",
    "    for i in range(len(test0)):\n",
    "        x = np.array(test0[i].flatten())\n",
    "        flat_test0 = np.append(flat_test0, x)\n",
    "    \n",
    "    for i in range(len(test1)):\n",
    "        x = np.array(test1[i].flatten())\n",
    "        flat_test1 = np.append(flat_test1, x)\n",
    "\n",
    "        \n",
    "    flat_test0 = flat_test0.reshape([980,784])\n",
    "    label0 = np.repeat(0,len(flat_test0))\n",
    "    label0 = label0.reshape([980,1])\n",
    "    flat_test0 = np.append(flat_test0,label0, axis = 1)\n",
    "    flat_test1 = flat_test1.reshape([1135,784])\n",
    "    label1 = np.repeat(1,len(flat_test1))\n",
    "    label1 = label1.reshape([1135,1])\n",
    "    flat_test1 = np.append(flat_test1,label1, axis = 1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    features1_0 = np.mean(flat_test0, axis = 1)\n",
    "    features2_0 = np.std(flat_test0, axis = 1)\n",
    "    features1_1 = np.mean(flat_test1, axis = 1)\n",
    "    features2_1 = np.std(flat_test1, axis = 1)\n",
    "    \n",
    "    #STEP4\n",
    "    ## calculating likelhood for feature 1 and feature 2\n",
    "    labels0 = []\n",
    "    labels1 = []\n",
    "    results0 = []\n",
    "    results1 = []\n",
    "    for i in range(len(flat_test0)):\n",
    "        p_f1_y0 = (1/pow(2*math.pi*my_pars['No.2'],0.5))*(1/math.exp(pow((features1_0[i]-my_pars['No.1']),2)/(2*my_pars['No.2'])))\n",
    "        p_f2_y0 = (1/pow(2*math.pi*my_pars['No.4'],0.5))*(1/math.exp(pow((features2_0[i]-my_pars['No.3']),2)/(2*my_pars['No.4'])))\n",
    "        p_f1_y1 = (1/pow(2*math.pi*my_pars['No.6'],0.5))*math.exp(-pow((features1_0[i]-my_pars['No.5']),2)/(2*my_pars['No.6']))\n",
    "        p_f2_y1 = (1/pow(2*math.pi*my_pars['No.8'],0.5))*math.exp(-pow((features2_0[i]-my_pars['No.7']),2)/(2*my_pars['No.8']))\n",
    "        \n",
    "        ### plugging in the likelihood and prior probabilities to find the posterior probability of each label\n",
    "        \n",
    "        p_y0_x = p_f1_y0*p_f2_y0*0.5\n",
    "        p_y1_x = p_f1_y1*p_f2_y1*0.5\n",
    "        \n",
    "        if (p_y0_x > p_y1_x):\n",
    "            x = 0\n",
    "        else:\n",
    "            x = 1\n",
    "        labels0 = np.append(labels0,x)\n",
    "        \n",
    "\n",
    "    labels0 = labels0.reshape([len(labels0),1])\n",
    "    flat_test0 = np.append(flat_test0,labels0,axis = 1)\n",
    "    \n",
    "    for i in range(len(flat_test0)):\n",
    "        if (flat_test0[i,-2] == flat_test0[i,-1]):\n",
    "            x = 1\n",
    "        else:\n",
    "            x = 0\n",
    "        results0 = np.append(results0,x)\n",
    "    accuracy0 = np.sum(results0)/980\n",
    "\n",
    "    for i in range(len(flat_test1)):\n",
    "        p_f1_y0 = (1/pow(2*math.pi*my_pars['No.2'],0.5))*(1/math.exp(pow((features1_1[i]-my_pars['No.1']),2)/(2*my_pars['No.2'])))\n",
    "        p_f2_y0 = (1/pow(2*math.pi*my_pars['No.4'],0.5))*(1/math.exp(pow((features2_1[i]-my_pars['No.3']),2)/(2*my_pars['No.4'])))\n",
    "        p_f1_y1 = (1/pow(2*math.pi*my_pars['No.6'],0.5))*math.exp(-pow((features1_1[i]-my_pars['No.5']),2)/(2*my_pars['No.6']))\n",
    "        p_f2_y1 = (1/pow(2*math.pi*my_pars['No.8'],0.5))*math.exp(-pow((features2_1[i]-my_pars['No.7']),2)/(2*my_pars['No.8']))\n",
    "\n",
    "        p_y0_x = p_f1_y0*p_f2_y0*0.5\n",
    "        p_y1_x = p_f1_y1*p_f2_y1*0.5\n",
    "        \n",
    "        if (p_y0_x > p_y1_x):\n",
    "            x = 0\n",
    "        else:\n",
    "            x = 1\n",
    "        labels1 = np.append(labels1,x)\n",
    "        \n",
    "\n",
    "    labels1 = labels1.reshape([len(labels1),1])\n",
    "    flat_test1 = np.append(flat_test1,labels1,axis = 1)\n",
    "    \n",
    "    #STEP5\n",
    "    ##Evaluating the model by calculating its accuracy\n",
    "    for i in range(len(flat_test1)):\n",
    "        if (flat_test1[i,-2] == flat_test1[i,-1]):\n",
    "            x = 1\n",
    "        else:\n",
    "            x = 0\n",
    "        results1 = np.append(results1,x)\n",
    "    accuracy1 = np.sum(results1)/1135\n",
    "    \n",
    "    my_pars['No.9'] = accuracy0\n",
    "    my_pars['No.10'] = accuracy1\n",
    "    \n",
    "    print(my_pars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi\n",
      "[5000, 5000, 980, 1135]\n",
      "Your trainset and testset are generated successfully!\n",
      "{'No.1': 44.17604719387755, 'No.2': 115.35043926449754, 'No.3': 87.40485379427022, 'No.4': 101.6027638232789, 'No.5': 19.349559948979593, 'No.6': 31.51353085364009, 'No.7': 61.31316505604545, 'No.8': 83.15106346182196, 'No.9': 0.9173469387755102, 'No.10': 0.9233480176211454}\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAO2ElEQVR4nO3df5DU9X3H8deb44QOQeQk4BVRaYpUmk6IOSEtDiXj6CCdDNqOrUxLaWvEmcpM6BCnDE6j02kSmmnUpJMfhcBIFU1sDYFOaSMltjYxQU5LBEobCEVFLlByTsBa4bh794/72jlwv589dr+734X38zFzs7vf9373+2a5131397Pf78fcXQAufiPKbgBAcxB2IAjCDgRB2IEgCDsQxMhmbuwSG+WjNaaZmwRCeVv/o9N+yirV6gq7mc2X9HlJbZK+6u6rU/cfrTGabTfVs0kACTt8e26t5pfxZtYm6YuSbpU0Q9IiM5tR6+MBaKx63rPPknTA3Q+6+2lJX5O0sJi2ABStnrBPlvTakNuHs2VnMbOlZtZtZt19OlXH5gDUo56wV/oQ4F3fvXX3Ne7e5e5d7RpVx+YA1KOesB+WNGXI7SslHamvHQCNUk/Yd0qaZmZTzewSSXdK2lJMWwCKVvPQm7ufMbNlkr6lwaG39e6+t7DOABSqrnF2d98qaWtBvQBoIL4uCwRB2IEgCDsQBGEHgiDsQBCEHQiCsANBEHYgCMIOBEHYgSAIOxAEYQeCIOxAEIQdCIKwA0EQdiAIwg4EQdiBIAg7EARhB4Ig7EAQTZ2yGReeUwtuSNb/9q8eSdbHjRhd87an/9PdyfrYf0s/9hWPPF/zti9G7NmBIAg7EARhB4Ig7EAQhB0IgrADQRB2IAhz96Zt7FLr8Nl2U9O2B+l/b5uVrLcv+3Gy/vXpTybrY0dcct49FeVtP5Osb35zSm7tid+Zn1zXu/fU1FPZdvh2nfBeq1Sr60s1ZnZI0klJ/ZLOuHtXPY8HoHGK+AbdR9z9eAGPA6CBeM8OBFFv2F3SM2b2opktrXQHM1tqZt1m1t2nU3VuDkCt6n0ZP8fdj5jZREnbzOw/3P25oXdw9zWS1kiDH9DVuT0ANaprz+7uR7LLY5I2SUp/9AugNDWH3czGmNnYd65LukXShTleAQRQz8v4SZI2mdk7j/OEu/9jIV3hLG/9+uxk/Scz2nJrf3f3Z5PrXjlyVLI+Quljxgc0kKw30mhL//r+1tie3NoD9/xMct1ru2tqqaXVHHZ3PyjpAwX2AqCBGHoDgiDsQBCEHQiCsANBEHYgCE4l3QQjr84/1FKSTq9Pr7/u5x9K1q8emTrMND20FtXn521M1r+oa5vUSfOwZweCIOxAEIQdCIKwA0EQdiAIwg4EQdiBIBhnL8Bbt6cPQb3zU1uT9Y+NO1hlC+WdrhkXD/bsQBCEHQiCsANBEHYgCMIOBEHYgSAIOxAE4+zD1D/v+txa54oDyXWrj6M3TrVpjf/kx3OT9Zc+k//vlqSjs9L7i/sXPp1bWzT29eS6jXRZ21vJ+shrrkrWzxx6tch2moI9OxAEYQeCIOxAEIQdCIKwA0EQdiAIwg4EYe7etI1dah0+225q2vbOx8grJiXr9313W27tl0efKrqds/T2px//M8c+klt74ZEPJdcd9/j3a+ppuNqum5Zbm7D+aHLdT09OnwfgvW2NOyf+iiM3Juv7b2js/3mtdvh2nfBeq1Srumc3s/VmdszM9gxZ1mFm28xsf3Y5vsiGARRvOC/jH5U0/5xlKyVtd/dpkrZntwG0sKphd/fnJPWes3ihpA3Z9Q2Sbiu2LQBFq/UDuknu3iNJ2eXEvDua2VIz6zaz7j615vscIIKGfxrv7mvcvcvdu9qZZBAoTa1hP2pmnZKUXR4rriUAjVBr2LdIWpJdXyJpczHtAGiUquPsZvakpHmSJkg6KukBSd+U9JSkqyS9KukOdz/3Q7x3KXOcve3yjmR90t/3JetfmfLtIts5S/eptmT995+6N1mfuvJ7RbbTMn762x9O1q9fvitZf/hn/7XmbZ8cOJ2s/9qqTyTrlz1Wzv9Japy96skr3H1RTqk1vx0DoCK+LgsEQdiBIAg7EARhB4Ig7EAQYU4lvf++6cn6pilfaNi2Vx//QLK+447rkvWpP7w4h9aqGbcxffjtK99Nn+559eb8533lhB8k1x07Ij1N9hszkmVdli6Xgj07EARhB4Ig7EAQhB0IgrADQRB2IAjCDgQRZpx99tx9DXvsR3rTg64v3H5tst5/8EdFthNGtWmTt/X8Qm6t2jj7xYg9OxAEYQeCIOxAEIQdCIKwA0EQdiAIwg4EEWac/fFr/jlZ7/Pa/+49+jc3J+tXHXy+5sdG7cZ8ckxubcSmOvdzFU/W3NrYswNBEHYgCMIOBEHYgSAIOxAEYQeCIOxAEGHG2fu8P1kf0ECyfsPO382tXfVnO2rqCY3Vc+PY3Fq1/++q0jOdt6Sqe3YzW29mx8xsz5BlD5rZ62a2K/tZ0Ng2AdRrOC/jH5U0v8Lyh919Zvaztdi2ABStatjd/TlJvU3oBUAD1fMB3TIzezl7mT8+705mttTMus2su0+n6tgcgHrUGvYvS3qfpJmSeiR9Lu+O7r7G3bvcvatdo2rcHIB61RR2dz/q7v3uPiBpraRZxbYFoGg1hd3MOofcvF3Snrz7AmgNVcfZzexJSfMkTTCzw5IekDTPzGZqcLTxkKR7Gtdia+jrSzxVA+kxfDTGyCsnJ+u3LuY8AkNVDbu7L6qweF0DegHQQHxdFgiCsANBEHYgCMIOBEHYgSDCHOJar6/fsDa3dvfiP0que9lj3yu6nRBGXj0lWR+z8a1k/U8n7iyynQsee3YgCMIOBEHYgSAIOxAEYQeCIOxAEIQdCCLMOPvGk53J+qKxryfr09vbcmvvv3d3ct2ef7g8We8//pNk/UL29kfzz2syevmR5LoLOncl6x8bd7CWlobl8Jn0KdSmre1J1s8U2UxB2LMDQRB2IAjCDgRB2IEgCDsQBGEHgiDsQBBhxtk/9c3fSNYXLf5CzY/9pSnPJuurvjU7Wf/2ul9J1scfOH3ePRVlwif/K1kfcEvWH736odxaR1t5MwT9dCD9nN668b5kferBC+8cBezZgSAIOxAEYQeCIOxAEIQdCIKwA0EQdiCIMOPso95Ijwf3eXra5XbLP569mk9fsSN9h/ur1Es0osr+YEADVR6hvLH03v78Y9JXvPbR5LpTV1144+jVVN2zm9kUM3vWzPaZ2V4z+3i2vMPMtpnZ/uxyfOPbBVCr4byMPyNphbtfJ+nDku41sxmSVkra7u7TJG3PbgNoUVXD7u497v5Sdv2kpH2SJktaKGlDdrcNkm5rUI8ACnBeH9CZ2TWSPihph6RJ7t4jDf5BkDQxZ52lZtZtZt19Sp/XC0DjDDvsZvYeSU9LWu7uJ4a7nruvcfcud+9qL/HDGiC6YYXdzNo1GPSN7v6NbPFRM+vM6p2SjjWmRQBFqDr0ZmYmaZ2kfe4+9HjFLZKWSFqdXW5uSIcFmfznzyfrvzR9WbK+65a/zK2NtjAjmC2l2mGqv/pE/mGqU1defENr1Qznt3SOpMWSdpvZrmzZKg2G/Ckzu0vSq5LuaEiHAApRNezu/h1Jed9IuanYdgA0Cl+XBYIg7EAQhB0IgrADQRB2IAgGiDPX/kF3sj53+Yrc2v1/uDG57sIxx2vq6WJwMjEWfrQ/va/Z+Eb6FNzPfGlOsj51bbyx9BT27EAQhB0IgrADQRB2IAjCDgRB2IEgCDsQhLl70zZ2qXX4bLv4DpSzD/1isn7wE+mvM+ye+9Ui2ynUXa/cnKy/8C/XJesde/N/v8Y9/v2aekK+Hb5dJ7y34lGq7NmBIAg7EARhB4Ig7EAQhB0IgrADQRB2IAjG2YGLCOPsAAg7EAVhB4Ig7EAQhB0IgrADQRB2IIiqYTezKWb2rJntM7O9ZvbxbPmDZva6me3KfhY0vl0AtRrOJBFnJK1w95fMbKykF81sW1Z72N3/onHtASjKcOZn75HUk10/aWb7JE1udGMAinVe79nN7BpJH5S0I1u0zMxeNrP1ZjY+Z52lZtZtZt19OlVftwBqNuywm9l7JD0tabm7n5D0ZUnvkzRTg3v+z1Vaz93XuHuXu3e1a1T9HQOoybDCbmbtGgz6Rnf/hiS5+1F373f3AUlrJc1qXJsA6jWcT+NN0jpJ+9z9oSHLO4fc7XZJe4pvD0BRhvNp/BxJiyXtNrNd2bJVkhaZ2UxJLumQpHsa0B+Aggzn0/jvSKp0fOzW4tsB0Ch8gw4IgrADQRB2IAjCDgRB2IEgCDsQBGEHgiDsQBCEHQiCsANBEHYgCMIOBEHYgSAIOxBEU6dsNrP/lvTKkEUTJB1vWgPnp1V7a9W+JHqrVZG9Xe3u761UaGrY37Vxs2537yqtgYRW7a1V+5LorVbN6o2X8UAQhB0Iouywryl5+ymt2lur9iXRW62a0lup79kBNE/Ze3YATULYgSBKCbuZzTez/zSzA2a2sowe8pjZITPbnU1D3V1yL+vN7JiZ7RmyrMPMtpnZ/uyy4hx7JfXWEtN4J6YZL/W5K3v686a/ZzezNkk/lHSzpMOSdkpa5O7/3tRGcpjZIUld7l76FzDMbK6kNyX9tbu/P1v2WUm97r46+0M53t3/uEV6e1DSm2VP453NVtQ5dJpxSbdJ+j2V+Nwl+vpNNeF5K2PPPkvSAXc/6O6nJX1N0sIS+mh57v6cpN5zFi+UtCG7vkGDvyxNl9NbS3D3Hnd/Kbt+UtI704yX+twl+mqKMsI+WdJrQ24fVmvN9+6SnjGzF81sadnNVDDJ3XukwV8eSRNL7udcVafxbqZzphlvmeeulunP61VG2CtNJdVK439z3P16SbdKujd7uYrhGdY03s1SYZrxllDr9Of1KiPshyVNGXL7SklHSuijInc/kl0ek7RJrTcV9dF3ZtDNLo+V3M//a6VpvCtNM64WeO7KnP68jLDvlDTNzKaa2SWS7pS0pYQ+3sXMxmQfnMjMxki6Ra03FfUWSUuy60skbS6xl7O0yjTeedOMq+TnrvTpz9296T+SFmjwE/kfSbq/jB5y+vo5ST/IfvaW3ZukJzX4sq5Pg6+I7pJ0uaTtkvZnlx0t1NtjknZLelmDweosqbcbNfjW8GVJu7KfBWU/d4m+mvK88XVZIAi+QQcEQdiBIAg7EARhB4Ig7EAQhB0IgrADQfwf035mF2fjwkcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
